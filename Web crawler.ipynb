{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A web crawler is a software program which browses the World Wide Web in a methodical and automated manner. It collects documents by recursively fetching links from a set of starting pages. Many sites, particularly search engines, use web crawling as a means of providing up-to-date data. Search engines download all the pages to create an index on them to perform faster searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To test web pages and links for valid syntax and structure.\n",
    "To monitor sites to see when their structure or contents change.\n",
    "To maintain mirror sites for popular Web sites.\n",
    "To search for copyright infringements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scalability: Our service needs to be scalable such that it can crawl the entire Web and can be used to fetch hundreds of millions of Web documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extensibility: Our service should be designed in a modular way with the expectation that new functionality will be added to it. There could be newer document types that needs to be downloaded and processed in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Some Design Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Infinite loop might occur\n",
    "* Is it a crawler for HTML pages only? Or should we fetch and store other types of media, such as sound files, images, videos, etc.? This is important because the answer can change the design. If we are writing a general-purpose crawler to download different media types, we might want to break down the parsing module into different sets of modules: one for HTML, another for images, or another for videos, where each module extracts what is considered interesting for that media type.\n",
    "* What protocols are we looking at? HTTP? What about FTP links? What different protocols should our crawler handle? For the sake of the exercise, we will assume HTTP. Again, it shouldn’t be hard to extend the design to use FTP and other protocols later.\n",
    "* What is the expected number of pages we will crawl? How big will the URL database become? Let’s assume we need to crawl one billion websites. Since a website can contain many, many URLs, let’s assume an upper bound of 15 billion different web pages that will be reached by our crawler.\n",
    "* What is ‘RobotsExclusion’ and how should we deal with it? Courteous Web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web crawler to fetch a special document called robot.txt which contains these declarations from a Web site before downloading any real content from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity Estimation and Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to crawl 15 billion pages within four weeks, how many pages do we need to fetch per second?\n",
    "\n",
    "15B / (4 weeks * 7 days * 86400 sec) ~= 6200 pages/sec\n",
    "What about storage? Page sizes vary a lot, but, as mentioned above since, we will be dealing with HTML text only, let’s assume an average page size of 100KB. With each page, if we are storing 500 bytes of metadata, total storage we would need:\n",
    "\n",
    "15B * (100KB + 500) ~= 1.5 petabytes\n",
    "Assuming a 70% capacity model (we don’t want to go above 70% of the total capacity of our storage system), total storage we will need:\n",
    "\n",
    "1.5 petabytes / 0.7 ~= 2.14 petabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic algorithm executed by any Web crawler is to take a list of seed URLs as its input and repeatedly execute the following steps.\n",
    "\n",
    "Pick a URL from the unvisited URL list.\n",
    "Determine the IP Address of its host-name.\n",
    "Establish a connection to the host to download the corresponding document.\n",
    "Parse the document contents to look for new URLs.\n",
    "Add the new URLs to the list of unvisited URLs.\n",
    "Process the downloaded document, e.g., store it or index its contents, etc.\n",
    "Go back to step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Breadth-first search (BFS) is usually used. However, Depth First Search (DFS) is also utilized in some situations, such as, if your crawler has already established a connection with the website, it might just DFS all the URLs within this website to save some handshaking overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path-ascending crawling: Path-ascending crawling can help discover a lot of isolated resources or resources for which no inbound link would have been found in regular crawling of a particular Web site. In this scheme, a crawler would ascend to every path in each URL that it intends to crawl. For example, when given a seed URL of http://foo.com/a/b/page.html, it will attempt to crawl /a/b/, /a/, and /."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Main problem is \n",
    "    - Large number voluem of web pages\n",
    "    - Rate change on web pages : Page can be changed after the visit or new page can be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/web1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. URL frontier: To store the list of URLs to download and also prioritize which URLs should be crawled first.\n",
    "2. HTTP Fetcher: To retrieve a web page from the server.\n",
    "3. Extractor: To extract links from HTML documents.\n",
    "4. Duplicate Eliminator: To make sure the same content is not extracted twice unintentionally.\n",
    "5. Datastore: To store retrieved pages, URLs, and other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Component Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let’s assume our crawler is running on one server and all the crawling is done by multiple working threads where each working thread performs all the steps needed to download and process a document in a loop.\n",
    "* The first step of this loop is to remove an absolute URL from the shared URL frontier for downloading. An absolute URL begins with a scheme (e.g., “HTTP”) which identifies the network protocol that should be used to download it. We can implement these protocols in a modular way for extensibility, so that later if our crawler needs to support more protocols, it can be easily done. Based on the URL’s scheme, the worker calls the appropriate protocol module to download the document. After downloading, the document is placed into a Document Input Stream (DIS). Putting documents into DIS will enable other modules to re-read the document multiple times.\n",
    "* Once the document has been written to the DIS, the worker thread invokes the dedupe test to determine whether this document (associated with a different URL) has been seen before. If so, the document is not processed any further and the worker thread removes the next URL from the frontier.\n",
    "![](images/web2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, our crawler needs to process the downloaded document. Each document can have a different MIME type like HTML page, Image, Video, etc. We can implement these MIME schemes in a modular way, so that later if our crawler needs to support more types, we can easily implement them. Based on the downloaded document’s MIME type, the worker invokes the process method of each processing module associated with that MIME type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Furthermore, our HTML processing module will extract all links from the page. Each link is converted into an absolute URL and tested against a user-supplied URL filter to determine if it should be downloaded. If the URL passes the filter, the worker performs the URL-seen test, which checks if the URL has been seen before, namely, if it is in the URL frontier or has already been downloaded. If the URL is new, it is added to the frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The URL frontier: The URL frontier is the data structure that contains all the URLs that remain to be downloaded. We can crawl by performing a breadth-first traversal of the Web, starting from the pages in the seed set. Such traversals are easily implemented by using a FIFO queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since we’ll be having a huge list of URLs to crawl, we can distribute our URL frontier into multiple servers. Let’s assume on each server we have multiple worker threads performing the crawling tasks. Let’s also assume that our hash function maps each URL to a server which will be responsible for crawling it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our crawler should not overload a server by downloading a lot of pages from it.\n",
    "We should not have multiple machines connecting a web server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  our crawler can have a collection of distinct FIFO sub-queues on each server. Each worker thread will have its separate sub-queue, from which it removes URLs for crawling. When a new URL needs to be added, the FIFO sub-queue in which it is placed will be determined by the URL’s canonical hostname. Our hash function can map each hostname to a thread number. Together, these two points imply that, at most, one worker thread will download documents from a given Web server and also, by using FIFO queue, it’ll not overload a Web server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How big will our URL frontier be? The size would be in the hundreds of millions of URLs. Hence, we need to store our URLs on a disk. We can implement our queues in such a way that they have separate buffers for enqueuing and dequeuing. Enqueue buffer, once filled, will be dumped to the disk, whereas dequeue buffer will keep a cache of URLs that need to be visited; it can periodically read from disk to fill the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. The fetcher module: The purpose of a fetcher module is to download the document corresponding to a given URL using the appropriate network protocol like HTTP. As discussed above, webmasters create robot.txt to make certain parts of their websites off limits for the crawler. To avoid downloading this file on every request, our crawler’s HTTP protocol module can maintain a fixed-sized cache mapping host-names to their robot’s exclusion rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. Document input stream: Our crawler’s design enables the same document to be processed by multiple processing modules. To avoid downloading a document multiple times, we cache the document locally using an abstraction called a Document Input Stream (DIS).\n",
    "\n",
    "A DIS is an input stream that caches the entire contents of the document read from the internet. It also provides methods to re-read the document. The DIS can cache small documents (64 KB or less) entirely in memory, while larger documents can be temporarily written to a backing file.\n",
    "\n",
    "Each worker thread has an associated DIS, which it reuses from document to document. After extracting a URL from the frontier, the worker passes that URL to the relevant protocol module, which initializes the DIS from a network connection to contain the document’s contents. The worker then passes the DIS to all relevant processing modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Dedupe test: Many documents on the Web are available under multiple, different URLs. There are also many cases in which documents are mirrored on various servers. Both of these effects will cause any Web crawler to download the same document multiple times. To prevent processing of a document more than once, we perform a dedupe test on each document to remove duplication.\n",
    "\n",
    "To perform this test, we can calculate a 64-bit checksum of every processed document and store it in a database. For every new document, we can compare its checksum to all the previously calculated checksums to see the document has been seen before. We can use MD5 or SHA to calculate these checksums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How big would be the checksum store? If the whole purpose of our checksum store is to do dedupe, then we just need to keep a unique set containing checksums of all previously processed document. Considering 15 billion distinct web pages, we would need:\n",
    "\n",
    "15B * 8 bytes => 120 GB\n",
    "Although this can fit into a modern-day server’s memory, if we don’t have enough memory available, we can keep smaller LRU based cache on each server with everything backed by persistent storage. The dedupe test first checks if the checksum is present in the cache. If not, it has to check if the checksum resides in the back storage. If the checksum is found, we will ignore the document. Otherwise, it will be added to the cache and back storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL filters: The URL filtering mechanism provides a customizable way to control the set of URLs that are downloaded. This is used to blacklist websites so that our crawler can ignore them. Before adding each URL to the frontier, the worker thread consults the user-supplied URL filter. We can define filters to restrict URLs by domain, prefix, or protocol type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Domain name resolution: Before contacting a Web server, a Web crawler must use the Domain Name Service (DNS) to map the Web server’s hostname into an IP address. DNS name resolution will be a big bottleneck of our crawlers given the amount of URLs we will be working with. To avoid repeated requests, we can start caching DNS results by building our local DNS server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7. URL dedupe test: While extracting links, any Web crawler will encounter multiple links to the same document. To avoid downloading and processing a document multiple times, a URL dedupe test must be performed on each extracted link before adding it to the URL frontier.\n",
    "\n",
    "To perform the URL dedupe test, we can store all the URLs seen by our crawler in canonical form in a database. To save space, we do not store the textual representation of each URL in the URL set, but rather a fixed-sized checksum.\n",
    "\n",
    "To reduce the number of operations on the database store, we can keep an in-memory cache of popular URLs on each host shared by all threads. The reason to have this cache is that links to some URLs are quite common, so caching the popular ones in memory will lead to a high in-memory hit rate.\n",
    "\n",
    "How much storage we would need for URL’s store? If the whole purpose of our checksum is to do URL dedupe, then we just need to keep a unique set containing checksums of all previously seen URLs. Considering 15 billion distinct URLs and 4 bytes for checksum, we would need:\n",
    "\n",
    "15B * 4 bytes => 60 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use bloom filters for deduping? Bloom filters are a probabilistic data structure for set membership testing that may yield false positives. A large bit vector represents the set. An element is added to the set by computing ‘n’ hash functions of the element and setting the corresponding bits. An element is deemed to be in the set if the bits at all ‘n’ of the element’s hash locations are set. Hence, a document may incorrectly be deemed to be in the set, but false negatives are not possible.\n",
    "\n",
    "The disadvantage of using a bloom filter for the URL seen test is that each false positive will cause the URL not to be added to the frontier and, therefore, the document will never be downloaded. The chance of a false positive can be reduced by making the bit vector larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Checkpointing: A crawl of the entire Web takes weeks to complete. To guard against failures, our crawler can write regular snapshots of its state to the disk. An interrupted or aborted crawl can easily be restarted from the latest checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We should use consistent hashing for distribution among crawling servers. Consistent hashing will not only help in replacing a dead host, but also help in distributing load among crawling servers.\n",
    "\n",
    "All our crawling servers will be performing regular checkpointing and storing their FIFO queues to disks. If a server goes down, we can replace it. Meanwhile, consistent hashing should shift the load to other servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our crawler will be dealing with three kinds of data: 1) URLs to visit 2) URL checksums for dedupe 3) Document checksums for dedupe.\n",
    "\n",
    "Since we are distributing URLs based on the hostnames, we can store these data on the same host. So, each host will store its set of URLs that need to be visited, checksums of all the previously visited URLs and checksums of all the downloaded documents. Since we will be using consistent hashing, we can assume that URLs will be redistributed from overloaded hosts.\n",
    "\n",
    "Each host will perform checkpointing periodically and dump a snapshot of all the data it is holding onto a remote server. This will ensure that if a server dies down another server can replace it by taking its data from the last snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are many crawler traps, spam sites, and cloaked content. A crawler trap is a URL or set of URLs that cause a crawler to crawl indefinitely. Some crawler traps are unintentional. For example, a symbolic link within a file system can create a cycle. Other crawler traps are introduced intentionally. For example, people have written traps that dynamically generate an infinite Web of documents. The motivations behind such traps vary. Anti-spam traps are designed to catch crawlers used by spammers looking for email addresses, while other sites use traps to catch search engine crawlers to boost their search ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Frpom each page we can fetch snippet of the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If page is crawled recently it will be set in db with low priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have 10 billion URL * 100 character * 4 bytes = 4 TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Imagine simple solution we can store in hash or sort it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have to split data in to different machines\n",
    "* We can store URL using its hash and store in machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can store all data in one machine by dividing data in various file and the file is chosed by hash of url, so we only have to search specific file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Service crawls a list of urls:\n",
    "Generates reverse index of words to pages containing the search terms\n",
    "Generates titles and snippets for pages\n",
    "Title and snippets are static, they do not change based on search query\n",
    "User inputs a search term and sees a list of relevant pages with titles and snippets the crawler generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/web3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We'll assume we have an initial list of links_to_crawl ranked initially based on overall site popularity. If this is not a reasonable assumption, we can seed the crawler with popular sites that link to outside content such as Yahoo, DMOZ, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We'll use a table crawled_links to store processed links and their page signatures.\n",
    "\n",
    "We could store links_to_crawl and crawled_links in a key-value NoSQL Database. For the ranked links in links_to_crawl, we could use Redis with sorted sets to maintain a ranking of page links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Crawler Service processes each page link by doing the following in a loop:\n",
    "Takes the top ranked page link to crawl\n",
    "Checks crawled_links in the NoSQL Database for an entry with a similar page signature\n",
    "If we have a similar page, reduces the priority of the page link\n",
    "This prevents us from getting into a cycle\n",
    "Continue\n",
    "Else, crawls the link\n",
    "Adds a job to the Reverse Index Service queue to generate a reverse index\n",
    "Adds a job to the Document Service queue to generate a static title and snippet\n",
    "Generates the page signature\n",
    "Removes the link from links_to_crawl in the NoSQL Database\n",
    "Inserts the page link and signature to crawled_links in the NoSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PagesDataStore is an abstraction within the Crawler Service that uses the NoSQL Database:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class PagesDataStore(object):\n",
    "\n",
    "    def __init__(self, db);\n",
    "        self.db = db\n",
    "        ...\n",
    "\n",
    "    def add_link_to_crawl(self, url):\n",
    "        \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def remove_link_to_crawl(self, url):\n",
    "        \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def reduce_priority_link_to_crawl(self, url)\n",
    "        \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def extract_max_priority_page(self):\n",
    "        \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def insert_crawled_link(self, url, signature):\n",
    "        \"\"\"Add the given link to `crawled_links`.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def crawled_similar(self, signature):\n",
    "        \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Page is an abstraction within the Crawler Service that encapsulates a page, its contents, child urls, and signature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Page(object):\n",
    "\n",
    "    def __init__(self, url, contents, child_urls, signature):\n",
    "        self.url = url\n",
    "        self.contents = contents\n",
    "        self.child_urls = child_urls\n",
    "        self.signature = signature\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Crawler is the main class within Crawler Service, composed of Page and PagesDataStore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Crawler(object):\n",
    "\n",
    "    def __init__(self, data_store, reverse_index_queue, doc_index_queue):\n",
    "        self.data_store = data_store\n",
    "        self.reverse_index_queue = reverse_index_queue\n",
    "        self.doc_index_queue = doc_index_queue\n",
    "\n",
    "    def create_signature(self, page):\n",
    "        \"\"\"Create signature based on url and contents.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def crawl_page(self, page):\n",
    "        for url in page.child_urls:\n",
    "            self.data_store.add_link_to_crawl(url)\n",
    "        page.signature = self.create_signature(page)\n",
    "        self.data_store.remove_link_to_crawl(page.url)\n",
    "        self.data_store.insert_crawled_link(page.url, page.signature)\n",
    "\n",
    "    def crawl(self):\n",
    "        while True:\n",
    "            page = self.data_store.extract_max_priority_page()\n",
    "            if page is None:\n",
    "                break\n",
    "            if self.data_store.crawled_similar(page.signature):\n",
    "                self.data_store.reduce_priority_link_to_crawl(page.url)\n",
    "            else:\n",
    "                self.crawl_page(page)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We'll want to remove duplicate urls:\n",
    "\n",
    "For smaller lists we could use something like sort | unique\n",
    "With 1 billion links to crawl, we could use MapReduce to output only entries that have a frequency of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class RemoveDuplicateUrls(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield line, 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        total = sum(values)\n",
    "        if total == 1:\n",
    "            yield key, total\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Detecting duplicate content is more complex. We could generate a signature based on the contents of the page and compare those two signatures for similarity. Some potential algorithms are Jaccard index and cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pages need to be crawled regularly to ensure freshness. Crawl results could have a timestamp field that indicates the last time a page was crawled. After a default time period, say one week, all pages should be refreshed. Frequently updated or more popular sites could be refreshed in shorter intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Client sends a request to the Web Server, running as a reverse proxy\n",
    "The Web Server forwards the request to the Query API server\n",
    "The Query API server does the following:\n",
    "Parses the query\n",
    "Removes markup\n",
    "Breaks up the text into terms\n",
    "Fixes typos\n",
    "Normalizes capitalization\n",
    "Converts the query to use boolean operations\n",
    "Uses the Reverse Index Service to find documents matching the query\n",
    "The Reverse Index Service ranks the matching results and returns the top ones\n",
    "Uses the Document Service to return titles and snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ curl https://search.com/api/v1/search?query=hello+world\n",
    "\n",
    "{\n",
    "    \"title\": \"foo's title\",\n",
    "    \"snippet\": \"foo's snippet\",\n",
    "    \"link\": \"https://foo.com\",\n",
    "},\n",
    "{\n",
    "    \"title\": \"bar's title\",\n",
    "    \"snippet\": \"bar's snippet\",\n",
    "    \"link\": \"https://bar.com\",\n",
    "},\n",
    "{\n",
    "    \"title\": \"baz's title\",\n",
    "    \"snippet\": \"baz's snippet\",\n",
    "    \"link\": \"https://baz.com\",\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/web4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some searches are very popular, while others are only executed once. Popular queries can be served from a Memory Cache such as Redis or Memcached to reduce response times and to avoid overloading the Reverse Index Service and Document Service. The Memory Cache is also useful for handling the unevenly distributed traffic and traffic spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the data size and request load, the Reverse Index Service and Document Service will likely need to make heavy use sharding and replication.\n",
    "DNS lookup can be a bottleneck, the Crawler Service can keep its own DNS lookup that is refreshed periodically\n",
    "The Crawler Service can improve performance and reduce memory usage by keeping many open connections at a time, referred to as connection pooling\n",
    "Switching to UDP could also boost performance\n",
    "Web crawling is bandwidth intensive, ensure there is enough bandwidth to sustain high throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "inputURL = \"https://www.quora.com\"\n",
    " \n",
    "resultUrl = {inputURL:False}\n",
    "# key is a url we want. value is True or False. True means already crawled\n",
    " \n",
    "# from urllib import urlopen\n",
    "import urllib2\n",
    "import urlparse\n",
    "import time\n",
    "import pprint\n",
    " \n",
    "import BeautifulSoup # get html links\n",
    " \n",
    "def processOneUrl(url):\n",
    "    \"\"\"fetch URL content and update resultUrl.\"\"\"\n",
    "    try:                        # in case of 404 error\n",
    "        html_page = urllib2.urlopen(url)\n",
    "        soup = BeautifulSoup.BeautifulSoup(html_page)\n",
    "        for link in soup.findAll('a'):\n",
    "            fullurl = urlparse.urljoin(url, link.get('href'))\n",
    "            if fullurl.startswith(inputURL):\n",
    "                if (fullurl not in resultUrl):\n",
    "                    resultUrl[fullurl] = False\n",
    "        resultUrl[url] = True       # set as crawled\n",
    "    except:\n",
    "        resultUrl[url] = True   # set as crawled\n",
    " \n",
    "def moreToCrawl():\n",
    "    \"\"\"returns True or False\"\"\"\n",
    "    for url, crawled in iter(resultUrl.iteritems()):\n",
    "        if not crawled:\n",
    "            print \"moreToCrawl found {}\".format(url)\n",
    "            return url\n",
    "    return False\n",
    " \n",
    "while True:\n",
    "    toCrawl = moreToCrawl()\n",
    "    if not toCrawl:\n",
    "        break\n",
    "    processOneUrl(toCrawl)\n",
    "    time.sleep(2)\n",
    " \n",
    "pprint.pprint(resultUrl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For seed URL collect all kind of sites like food, travel, educational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now a days we have single page application and not all content are loaded we have to use renderer to fetch all content. Next.js or Gatsby provide such service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/web5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](images/web6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fron queue store URL priority wise. Frequent changing website or new website given higher priority\n",
    "* Back queue select url from front queues and put in back queues\n",
    "* One domain is only mapped to one back quque for politeness\n",
    "* back queueue = number of worker threads\n",
    "* Heap is used to determine min back queue which need to be processed, once that is processed we assgin some hiugher number to it so that we do not bombard other wev server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To detect update of page we can send HEAD request to just check last modified time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sim hash to detect duplicate\n",
    "    - Reomve stop words, lemmitization\n",
    "![](images/web7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate useful words binary version multiply it by its occurence count, replace 0 by the -1 and sum it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
