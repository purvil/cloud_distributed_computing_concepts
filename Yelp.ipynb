{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total 200 million user submited reviews and even more photos.\n",
    "* Feature name Popular Dishes\n",
    "    - shows restaurant’s most popular dishes, including the dish names, photos and review snippets.\n",
    "* The Popular Dishes feature highlights the most talked about and photographed dishes at a restaurant, gathering user opinions and images in one convenient place\n",
    "* Data\n",
    "    - Ranked list of popular menu items\n",
    "    - Reviews mentioning each item\n",
    "    - Photos associated with each item\n",
    "* Menu can be uploaded by owner, partners. Now problem is to match item to the review to the photos. May be simple python string comparision to NLP\n",
    "![](images/yelp/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many business without menu\n",
    "    - Inferred menu model: \n",
    "        1. Extract all potential menu items from reviews\n",
    "        2. aggregate those to create inferred menu\n",
    "    - Named Entity Recognition (NER) process. Classify each token in text by whether or not it belongs to a named entity. In our case, we classified each token in a review by whether or not it belonged to a menu item.\n",
    "    - Token item by S (single), I(irrelevant), B(begin), M(middle), E(end)\n",
    "    - Extract all S and BM*E\n",
    "    - To train we can use already available menu data. Doing hand labeling is costly and NLP requires lots of data.\n",
    "    - We also have to match fuzzy review which has short words for menu items, misspelling. Some time it create false positive, we get item which is not in menu\n",
    "    - Also false negative like there is a item in menu but in review mentioned in different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our focus was on achieving a high precision, as mistakes would look bad and hurt user trust. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are they actually dishes?\n",
    "* Are they popular?\n",
    "* Are we finding all their matches in photos and reviews? Are we matching anything we shouldn’t be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Popular Dishes backend is currently deployed as a handful of PySpark batches. Every day, all the data we have about our businesses is gathered and run through an NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yelp/7.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yelp/8.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoScaling AWS Step functions Activites\n",
    "* AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.\n",
    "* Step Functions provides a reliable way to coordinate components and step through the functions of your application. Step Functions offers a graphical console to visualize the components of your application as a series of steps. It automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected, every time. Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transactional ordering at Yelp covers a wide variety of verticals, including food (delivery/takeout orders), booking, home services, \n",
    "* These orders are processed via Step Functions, where each is represented as an execution instance of the workflow\n",
    "![](images/yelp/3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Activities are an AWS Step Functions feature that enables you to have a task in your state machine where the work is performed by a worker that can be hosted on Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS), mobile devices—basically anywhere.\n",
    "* Each step in the above workflow is an “activity,” and Yelp implements these activities as batch daemons, which interact with AWS Step Functions via an API integration that fetches tasks and submits activity execution results. We run multiple instances of these batch daemons,\n",
    "* By default, the Amazon States Language doesn't set timeouts in state machine definitions. Without an explicit timeout, Step Functions often relies solely on a response from an activity worker to know that a task is complete. If something goes wrong and TimeoutSeconds isn't specified, an execution is stuck waiting for a response that will never come.\n",
    "* Each activity within a workflow needs to be completed within a specified time, such that the sum of the time taken by all activities in a workflow is within the limits of the workflow timeout. We also use activity retries, which are configurable per activity, to achieve resiliency in cases of intermittent recoverable failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yelp/4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* each execution will be queued in the ActivityScheduled state until it’s picked up by one of the “activity workers.” Since the total time per activity (including execution and wait time) is bound, tasks with longer wait times get less time for execution. These tasks may need retries, and cascading effects from multiple activities could hit a workflow timeout threshold. As AWS provides aggregated metrics on the wait time for these tasks (ActivityScheduleTime), in order to maintain the desired success rate and latencies for workflow processing,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auto Scaling of Activity instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transactional flows at Yelp experience repetitive traffic patterns over the course of the day and week. Before the new autoscaling system, we used Step Functions cloudwatch metrics to tune the activity instances count to meet service-level objectives. In these cases, we provisioned a static count of activity workers to handle peak traffic, which led to a lot of unused compute capacity during non-peak hours, making transactional flow an ideal use case for auto-scaling.\n",
    "![](images/yelp/5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autoscaler first fetches the scaling configuration, then gathers scaling demands from the AWS side, computes the scaling decision, and, lastly, invokes the PaaSTA api to adjust the instance count.\n",
    "* We utilize Step Functions Metrics (ActivityScheduleTime metric) (The interval, in milliseconds, for which the activity stays in the schedule state.) and Cloudwatch Alarms to detect any scaling-worthy events, and SNS and SQS services for relaying scaling messages.\n",
    "    - Simple notification service\n",
    "    - Simple queue sevice\n",
    "* there are two cloudwatch alarms for each activity: scale up and scale down. When an alarm detects a breaching condition, it sends out “ALARM” notifications to be polled by the Autoscaler. It will then send an “OK” notification when the ActivityScheduleTime metric is back to normal.\n",
    "![](images/yelp/6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upon receiving scaling messages, the Autoscaler will validate the message, parse, and comprehend the request, and then compute a concrete scaling decision for a specific activity. It considers two major factors for scaling decisions: scaling configurations (e.g., min/max count and scaling gradient) and scaling record (e.g., the last alarming time and the last scaling time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When traffic ramps up, the scale-up alarm fires off an “ALARM” notification and the Autoscaler repeatedly scales up the activity workers until the ActivityScheduleTime metric is back to the normal threshold. When traffic settles down, the cloudwatch alarm sends out an “OK” notification. With the “OK” message, the autoscaler will begin to clean up the previous “ALARM” notification and wrap up the cycle of scaling for that activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Flapping (continuous churn of scale-up and -down events) is a typical challenge for any auto-scaling system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We support a scale-down cool-off time to prevent two consecutive scale-down actions within a certain amount of time. This value is configurable by service owners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conservative scaling down is based on historical statistics for scale-down alarms so that they’re less susceptible to triggers and never occur during peak hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We used historical data for the initial values of scale-up and scale-down threshold.\n",
    "* Be aggressive on scaling up and conservative on scaling down We used cool-down times, small scale-down gradients, and larger evaluation periods for conservative scale-downs, as opposed to large scale-up gradients and smaller evaluation periods for aggressive scale-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating Kafka's Zookeeper With No Downtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Behind the scenes, Kafka uses Zookeeper for various distributed coordination tasks, such as deciding which Kafka broker is in charge of assigning partition leaders and storing metadata about the topics in its brokers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zookeeper clusters are pretty lightweight. When possible, try not to share them between different services, as they may start to cause performance issues in Zookeeper, which are hard to debug and usually require downtime to repair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTL as service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* “TTL-as-a-Service” (Time-To-Live): a system to identify and flag users and their stale privileges.\n",
    "*  The premise is simple: if you haven’t used your access in X days, you probably don’t need it anymore and won’t notice if we take it away.\n",
    "* At its core, this requires two things:\n",
    "    - Knowledge of every time a user has used a given privilege.\n",
    "    - The ability to revoke access upon detecting staleness.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Collaborative Filtering with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A widely-adopted approach for building a collaborative filtering model is matrix factorization. The Spark ML library contains an implementation of a collaborative filtering model using matrix factorization based on the ALS (Alternative Least-Square) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since a user interacts with only a few of the tens of millions of businesses on Yelp, this matrix is sparse. The objective of the matrix factorization model is to decompose this sparse user-business matrix into a user matrix U (where each row represents a latent factor representation of that user, also known as user-embeddings) and a business matrix B (where each column represents a latent factor representation of that business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Users can express their interest in different businesses on Yelp through various interactions. These interactions vary and can be either explicit feedback, such as writing reviews or rating a business, or implicit feedback, like checking in at or bookmarking a business, adding it to a collection, viewing a business page, browsing through business photos, or using features like request a quote (RAQ). In our matrix factorization model, we used the implicit feedback signals because the amount of implicit feedback signals exceeds the amount of explicit signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The implicit feedback model has a few hyperparameters - rank or dimension of the latent space, regularization parameters, alpha or weight on the positive interaction terms. In order to determine the proper set of hyper-parameters, we performed grid-search hyper-parameter tuning over a subset of the input data. We chose the set of hyper-parameters that maximizes our desired metric in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once the matrix factorization model is trained, we can determine a user’s affinity or interest for a business as the inner-product of the user-embedding (u), and the business embedding (b) found from the model.\n",
    "\n",
    "user-business affinity-score = u ⋅ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *To serve the exact top-K business recommendations for all users, we need to calculate the affinity-scores for all (user, business) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because order history information is split across multiple tables, each request required a join between the Order table, the OrderLine table (order lines are the individual items in an order, like “2 samosas”), the Address table, and the CancellationPolicy table. This was becoming expensive, especially considering that the order history page was not the only consumer of this data. More and more features across Yelp needed to access order history and, in many cases, the service calls to fetch it were some of the slowest made by clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We realized that we had been asking MySQL to do double duty. We needed:\n",
    "\n",
    "A relational data store with ACID guarantees to power the complicated finite state machine underlying order processing.\n",
    "A distributed key-value store for quickly retrieving order history that could be used in latency-sensitive, high-traffic applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On top of this, we became increasingly interested in having a backend that acts as a general purpose search engine with full text search, allowing us to do things like find the most commonly ordered dishes for a restaurant, or even the most popular burger in a particular area.\n",
    "\n",
    "Although some of these problems, like speeding up the order history page, could have been solved by more aggressive MySQL indexing or materialized views, we knew these would be temporary solutions that wouldn’t solve our needs more broadly and wouldn’t grow with our future product goals. It became clear that we’d need a different tool to solve this new class of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we chose Elasticsearch because it could accomodate all of our use cases, from retrieving a single user’s order history, to full text search over order items, to fast geolocation queries. It is also widely deployed at Yelp as the backend for new search engines, so a lot of tooling and infrastructure for it already exists. The most attractive attribute was the flexibility to make new queries that we may not have anticipated when first designing the data model - something that’s much more difficult with datastores like Cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  we developed a schema that includes the high level order state, all the order lines, the user id, information about how the user entered the order flow, plus some high level partner details. We chose to exclude details of the business itself, all details of the user aside from the delivery address, and any details about the payment processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* how we’d funnel our MySQL data into it. The most straightforward approach might be a batch that periodically queries MySQL for new orders and uses the Elasticsearch REST API to insert them. This has its share of problems, including:\n",
    "\n",
    "The latency between an order being placed and being picked up by the batch.\n",
    "Lack of clarity for how to track updated orders since the last batch run without polluting our MySQL schemas with new flags to mark unflushed updates.\n",
    "Inability to quickly rebuild our data in Elasticsearch from scratch in the case of data loss or corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* were able to leverage Yelp’s realtime data pipeline, which persists incoming MySQL data to Kafka by scraping MySQL binary logs. This allowed us to build an asynchronous processing pipeline for Elasticsearch writes out of band with order processing itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One new difficulty that presented itself was how to make sense of many independent Kafka streams of change events, with each table’s changes written to a different Kafka topic. As mentioned above, orders go through multiple intermediary states, with state changes reflected across multiple tables and written out in large database transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching internal service call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Casper is a caching proxy designed to intercept traffic flowing between internal Yelp services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, as the number of services and traffic begin to scale, some of Yelp’s services experience a disproportionate amount of load. These are what we call “data” services, or “core” services, containing or processing core data like user, business or review-related information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our Memcached usage is fairly standard: we have a client library which lets applications interact with a set of Memcached clusters deployed in their AWS region. It’s a very thin wrapper around the Memcached protocol (“GET”, “SET”, “DELETE”, etc). Applications dictate cache keys, values, TTL (Time-To-Live), and invalidation strategies. This flexibility comes at a cost:\n",
    "\n",
    "As Yelp grows the number of services in its infrastructure, it’s extremely hard to understand which pieces of data are cached, how, and where.\n",
    "Tricky forward/backward compatibility of data in the cache with the code being deployed can lead to outages\n",
    "Updates to caching policies require deployments, because application code drives these decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In practice, each host at Yelp runs a local HAProxy instance to route requests to services. When a process wants to contact a service it connects to HAProxy locally, and requests are forwarded to available backend servers transparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The HAProxy instance running on each host is periodically updated and restarted to reflect backend servers going up or down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caching service calls is traditionally done either on the server or on the client, through a thin client library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of deploying Casper as infrastructure with an associated client or server library we designed, built and deployed Casper as a separate service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first is that changes or updates to Casper are completely independent from clients and underlying Yelp services. More importantly, Casper deployments are not special, because Casper is just another Yelp service.\n",
    "* Rather than routing to backend servers directly, HAProxy instances running on client hosts route requests to Casper first. As an added bonus, HAProxy does not forward requests to Casper if it’s down or unhealthy, allowing for a natural fail-safe mechanism.\n",
    "* CLient to casper get 1,2,3 and cache miss, go to server and then write to cache, return ans to client . Even partial and superset request also handled effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Invalidation can be done explicitly and TTL system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We also use this API to reactively invalidate resources instead of letting them expire naturally. A concrete example is user data, which we cache for 1hr. In the absence of reactive invalidation, any update to a Yelp profile (say, an update to your tagline) could take up to an hour to be reflected on your app. To address this concern we’ve built a system based our Data Pipeline to trigger invalidation requests when our database commits changes to our “user” table, bringing down the potential staleness from up to an hour to consistently less than a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Gradient Boosted Trees for CTR Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  we’ve built a click-through-rate (CTR) prediction model that determines whether or not to serve ads from a particular advertiser. The predicted CTR determines how relevant the business is to the user’s intention and how much would need to be bid to beat a competitor in the auction. It is crucial to predict the most accurate CTR as possible to ensure consumers have the best experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Previously, we used Spark MLlib to train a logistic regression model for CTR prediction. However, logistic regression is a linear classifier and cannot model complex interactions between covariates unless explicitly specified or engineered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consequently, we decided to use Gradient Boosted Trees (GBTs), a nonlinear classifier and a powerful machine learning technique that uses an ensemble of weak learners (decision trees, in this case) to train on the residuals of the previous classifiers and form a more accurate ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the sampling phase, we implemented the ScalableSRS algorithm to generate a train-and-test set from our web logs. ScalableSRS has better theoretical guarantees than Bernoulli Sampling and scales well to large datasets. We then convert the sampled logs into a sparse feature matrix for the feature extraction phase. After the model is trained, we evaluate it against the test set, generating offline metrics for analysis. For these data wrangling tasks, we rely on tools like mrjob, Apache Spark, and Apache Zeppelin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For model training, we use XGBoost, a popular library for training gradient boosted trees. Since our datasets are large, we used data parallelism techniques to train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Yelp's Core Business Search to Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suboptimal Support for Realtime Indexing\n",
    "Our legacy system used a master-slave architecture where masters were responsible for handling writes (indexing requests) and the slaves would serve live traffic. A master instance was responsible for taking a snapshot of the Lucene index and uploading it to S3 so that the slave could download it periodically and serve fresh data. Thus there was a delay in the updated index being served by the queries at search time. Some search features, such as reservations and transactions, could not afford this delay (a few minutes), and required the indexed data to be served instantly (within a few seconds). To solve this problem, we had to use another realtime store, Elasticsearch, and query it in parallel to the business store (legacy search store) which meant that the application service would then need to compute the final search results based on both results. As we grew, this approach did not scale well and we had to deal with performance issues arising from combining and sorting results in the application layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have a large team of developers working constantly on improving the ranking algorithm for the search results. We end up making code pushes to the underlying search ranking algorithm multiple times a day and with the legacy system, each code push took several hours. \n",
    "* Our data was large enough that it had to be sharded. We used a two-tier sharding approach.\n",
    "\n",
    "Geosharding: We split the businesses into separate logical indexes based on their location. For example, a business in San Francisco would be in a separate index from a business in Chicago because the shard might split down the middle of the country.\n",
    "Microsharding: We further split each geographical index into multiple “microindexes” or “microshards”. For this we used a simplistic mod-based approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each Lucene index-backed process was its own service instance. We had to account for replication as well to ensure availability. For example each <geoshard>_<microshard> would have multiple instances called “replicas” to guard against outages of instances. This meant we had a large number of service instances and each instance would take some time to start up since each instance needed to\n",
    "\n",
    "download tens of gigabytes of data from S3\n",
    "warm the Lucene index to preload Lucene caches\n",
    "compute and load various data sets into memory\n",
    "force garbage collection, since startup created a lot of ephemeral objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It starts with search requests coming into the coordinator service. This service figures out the corresponding geographical shard to use (based on the physical location of a business) and forwards the request to the appropriate shard, in this simplified case either west or east. The request would be broadcast to all the microshards (2nd tier of sharding for horizontal scaling) within that geographical shard. After getting the result from one to N microshards the coordinator would combine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yelp/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yelp/10.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* A search request is transformed into a Lucene query, which is then sent to the Lucene index. Lucene returns a stream of results as per the collector specified. One can think of the collector as the ranker, the thing deciding the order of the results. This is where the ranking logic is applied. Ranking logic at Yelp uses a number of heuristics to determine the end result ranking. These heuristics include looking into certain business related data\n",
    "\n",
    "* Business Field cache: forward index for a business (things like business attributes)\n",
    "* Top Query Info: Data derived from our user activity\n",
    "* Misc data: includes Yelp-specific data like Yelp categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need\n",
    "    - decoupling of application logic from the backend used\n",
    "faster code pushes\n",
    "ease of storing custom data and forward indexes that powered the search results ranking (e.g. context specific data)\n",
    "real-time indexing\n",
    "linear performance scalability with respect to growth of our business data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We’re continuously adding new features to our API to make it easier for developers to integrate with our data and share great local businesses through their apps. Today, we’re releasing access to query our data via GraphQL, a graph query language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GraphQL is a query language for APIs that places emphasis on being able to query for exactly the data you want. I’m sure most of you at some point have thought, “I really wish this endpoint had more data,” or, “I only need one or two pieces of data from this huge API response,” or, “I wish I could make one API call instead of 2 or 3 to get the data I want.” GraphQL gives you the flexibility to control exactly what data you get back from our API! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  business(id: \"garaje-san-francisco\") {\n",
    "    name\n",
    "    url\n",
    "    photos\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"data\": {\n",
    "    \"business\": {\n",
    "      \"name\": \"Garaje\",\n",
    "      \"url\": \"https://yelp.com/biz/garaje-san-francisco\",\n",
    "      \"photos\": [\n",
    "        \"https://s3-media2.fl.yelpcdn.com/bphoto/_nN_DhLXkfwEkwPNxne9hw/o.jpg\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the name implies, GraphQL also makes traversing graphs (and therefore relational data) very easy. Unlike most REST APIs, you don’t need to make multiple requests to pull relational data. Based on the schema, you can retrieve data based on the relations they have. For our API users, this means easy access to all the great business information we have available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How We Scaled Our Ad Analytics with Apache Cassandra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  We have over 100,000 paying advertisers. Every day, we calculate the numbers of views and clicks each ad campaign received the previous day and the amount of money spent by each campaign. With these analytics, we generate bills and many different types of reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We didn’t need the benefits of traditional relational databases because we don’t have a transactional workload or a complex data model that would require joining tables. For us, the main advantages of Cassandra were simple scaling of storage, flexible schemas, and high write performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Growing our ad business means more campaigns, which means more analytics data. Because Cassandra is designed to be distributed, it’s easy to increase storage space - all you have to do is add more nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our data requirements change over time. For example, we recently decided to start breaking down analytics information by ad type like mobile search or desktop review. With Cassandra’s flexible schemas, we can easily add new columns to our data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cassandra is well-known for its impressive write capacity. In one benchmark test, Netflix achieved over 1 million writes per second! As we increase the amount of analytics data we write each day, we need to be sure that our system can handle it quickly, especially because we need the latest data available for daily reports and billing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
