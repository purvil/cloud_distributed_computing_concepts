{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cloud computing - on demand computing and lots of data is fuel for big data technology.\n",
    "* Personalized marketing.\n",
    "* Recommendation engine\n",
    "* Sentiment analysis (opinion mining)\n",
    "* Personalized treatment (Precision medicine)\n",
    "* IOT/ smart cities\n",
    "* Machine Data\n",
    "    - Sensor data\n",
    "    - fitness device data, \n",
    "* Organizational data\n",
    "    - government data,\n",
    "    - static/ updated at slow rate\n",
    "    - commercial transaction, banking records, e-commerce, medical records, government data\n",
    "* People\n",
    "    - social media data\n",
    "    - blog, comments, \n",
    "    - Text heavy and unstructured data (no pre define data model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Big Data.\n",
    "#### Volume\n",
    "* Vast amount of data\n",
    "* challenge of volume is, storage, fast enough data acquisition\n",
    "*\n",
    "#### Velocity\n",
    "* Speed at which data is generated\n",
    "* Personalized advertisement, late analysis -> missing opportunity.\n",
    "* Real time vs batch processing\n",
    "    - In batch processing large amount of data -> clean data -> feed in chunks for analysis -> wait for result\n",
    "    - In real time processing, instantly capture streaming data -> feed real time to machine -> process real time -> act\n",
    "\n",
    "#### variety\n",
    "* Different form of data text, images, voice, geo-spatial\n",
    "* Data used to be in table, but now it is heterogeneous.\n",
    "* Structural variety : formats and models\n",
    "* Media variety : medium in which data get delivered  Transcript vs mp3\n",
    "* Semantic variety : how to interpret and operate on data. Age can be number or category (adult)\n",
    "\n",
    "### Veracity\n",
    "* Bias, noise and abnormality in data\n",
    "* Uncertainty in data\n",
    "* Junk in = junk out\n",
    "* Accuracy of data\n",
    "* Reliability of data source\n",
    "* How data is generated.\n",
    "* Over estimation or under estimation can occur if data is not accurate.\n",
    "\n",
    "### Valance\n",
    "* Connectedness of data\n",
    "* More connected data higher the valance.\n",
    "* valance is actual connection / total possible connection\n",
    "\n",
    "### Value\n",
    "* How to use big data in correct way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data science process:\n",
    "* Acquire data\n",
    "    - identify data set, retrieve it, query data\n",
    "    - Traditional database use SQL\n",
    "    - Text data use scripting language\n",
    "    - Remote data - Web services such as REST (representational state transfer), SOAP, websocket (allow real time notification from website). data can be XML, JSON\n",
    "    - NOSQL data store - API, web service interface . REST\n",
    "* Prepare data\n",
    "    - Look for corelation(dependency of variables) trends(is there consistent direction data is moving), outlier (error, rare event)\n",
    "    - Summary statistics (mean,median, range, SD)\n",
    "    - visualize data (heat map(where the hot spot are), histogram (skewness, unusual dispersion), box plot (data distribution), line plot (how the data is changing over time, spike in data is easy to spot), scatter plot(correlation).\n",
    "    - Explore and pre-process it\n",
    "    - Understand nature of data\n",
    "    - cleaning subset filter data\n",
    "    - Integrate multiple data\n",
    "    - Inconsistent value (2 different address)\n",
    "    - Duplicate records\n",
    "    - missing value\n",
    "    - invalid data (6 digit zip code)\n",
    "    - Remove missing data\n",
    "    - merge duplicate record\n",
    "    - Generate best estimate for invalid value\n",
    "    - Remove outlier.\n",
    "    - Dimensionality reduction(PCA), transformation(reduce noise and variability, aggregate data like daily sales record can have variability combine them as monthly or weekly record), feature selection(remove, combine, add features. purchase price and tax paid is correlated so we can remove one.), scaling ( changing range of value between specific range 0 to 1. avoid dominating large values, comparing height and weight, weight can dominate value. changing both 0 and 1 can equalize contribution of both)\n",
    "    - correctly format data\n",
    "* Analyze data\n",
    "    - Select analytical technique\n",
    "    - Build mode\n",
    "    - Classification(sunny, windy, rainy, cloudy), regression(predict numeric value),  clustering(organize similar item in groups), association analysis(Find set of rules to capture association between items, market basket analysis. Checking account and CD account, dyper beer connection) and graph analysis (Use graph structure to find connection between entities)\n",
    "    - Select technique, build model, validate model \n",
    "    - predicted value = correct value\n",
    "    - Clustering - does group make sense.\n",
    "* Communicate \n",
    "    - What to present\n",
    "    - What is domain result,  what added value this result provide. How result compare to criteria we decided at start.\n",
    "    - Scatter, line heat map\n",
    "    - tables\n",
    "    - Tableau, Python packages D3, google charts, timeline\n",
    "* Apply result\n",
    "    - What action should taken based on insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed File system\n",
    "* Many storage computer connected over network is called distributed file system\n",
    "\n",
    "### Scalable computing/programming\n",
    "* Parallel process block of data.\n",
    "* Programmability on top of distributed file system\n",
    "* Split volume of data\n",
    "* Access data fast Distribution computing to node\n",
    "* handle fault tolerance\n",
    "* Enable adding new resources (scaling out)\n",
    "* MapReduce programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop\n",
    "* Enable scalability to store large volume of data\n",
    "* Handle fault tolerance\n",
    "* optimized for variety data types\n",
    "* Facilitate a shared environment. at the same time node can run multiple different jobs.\n",
    "![](images/hadoop.jpg)\n",
    "\n",
    "* HDFS : Scalable and reliable storage. Enable scaling out. Fault tolerance\n",
    "* YARN : scheduling and resource manager over HDFS storage\n",
    "* Mapreduce : programming model, give 2 function instead of dealing with parallel programs\n",
    "* Hive : SQL like queries at facebook\n",
    "* Pig: dataflow based scripting at yahoo\n",
    "* Giraph : process large scale graph\n",
    "* Storm, spark, flink : real time and in memory processing of big data.\n",
    "* Hbase\n",
    "* Cassandra\n",
    "* MongoDB\n",
    "* Zookeeper : centralized management system, synchronization, configuration and availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDFS\n",
    "* Provides scalability, reliability.\n",
    "* Add more node to increase storage.\n",
    "* Split files across nodes for parallel access. Default chunk size 64 MB\n",
    "* Replication for fault tolarance. (3 copy by default)\n",
    "* Handle variety of type\n",
    "* Namenode for metadata, Datanode for block storage.\n",
    "* One name node per cluster, 1 datanode per machine\n",
    "* name node is  admin of HDFS cluster, it coordinate operations. Keeps track of file name, location. map content to data node.\n",
    "* Data node is responsible for storing file blocks\n",
    "* Data node listen command from name node for block creation, deletion, replication.\n",
    "* Replication provides fault tolerance and data locality.\n",
    "\n",
    "\n",
    "#### YARN\n",
    "* Resoure manager\n",
    "* interact with application and schedule them for optimal performance\n",
    "* YARN run multiple application on HDFS.\n",
    "* Original Hadoop has no YARN. not able to run non map reduce. when different kind of analysis needed like graph analysis, we had to move data to different platform.\n",
    "* YARN unable new ways to use data from HDFS\n",
    "* Resource manager controls resources and decides who gets what\n",
    "* node manager is incharge of single machine.\n",
    "* Each application gets application master. It negotiate resources from resource manager and talks to node manager to complete task. Container is like computing machine.\n",
    "\n",
    "\n",
    "#### MapReduce\n",
    "* Programming model for hadoop eco system\n",
    "* Using YARN it schedule parallel processing.\n",
    "* Map = apply operation to all elements\n",
    "* Reduce = Summarize operation on elements.\n",
    "* If data is frequently changing mapreduce will not work. As map reduce reads entire data before performing operation\n",
    "* Map and reduce should be independent.\n",
    "* Not good for interactive operation.\n",
    "* Useful for search engine ranking and topic mapping\n",
    "\n",
    "#### When to use Hadoop?\n",
    "* Future anticipated data growth.\n",
    "* Long term availability of data vs putting data in archive storage.\n",
    "* Many application over same data store\n",
    "* High volume, high variety\n",
    "* Small dataset : think twice before using hadoop\n",
    "* Good for data parallelism, same task simultaneously on multiple different data. Task parallelism is multiple different task on same data.\n",
    "* Random data access is hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Management\n",
    "* How do we ingest data?\n",
    "    - How many data source?\n",
    "    - How large data items?\n",
    "    - Will number of data source grow?\n",
    "    - Rate of data ingestion\n",
    "    - What to do with bad data?\n",
    "    - What to do when data is too little or to much?\n",
    "    - Hospital information system\n",
    "        - Some data like medical image is large\n",
    "        - Medical records are small in size but number of records are large.\n",
    "        - Number of data sources is stable.\n",
    "        - Number of data ingestion is proportional to patient activity\n",
    "        - Data can never be discarded.\n",
    "        - Error handling, warn, flag\n",
    "    - Cloud dataset of personal information\n",
    "        - Data sources will be people who have subscribed for service\n",
    "        - Data rate is fast. large volume\n",
    "        - To cop up with data rate, erroneous data retry once and then will be discarded.\n",
    "* Where and how do we store it?\n",
    "    - How much data to store?\n",
    "    - Storage device attached to computer (fast access) or network attached? (allows scale up easily). \n",
    "    - How fast do we need to read and write?\n",
    "* How to ensure data quality?\n",
    "* What operation do we perform on data?\n",
    "* How to make operation efficient?\n",
    "* How to scale up data volume, variety, velocity, velocity and access?\n",
    "    -Vertical scaling/ scaling up: adding more processing power and RAM, buying more expensive and robust server. Expensive. \n",
    "    - Scaling out, horizontal scaling : Adding more, less powerful machine that interconnect over network. Easier to practice to add more machines.\n",
    "    \n",
    "* How to keep data secure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NY smart meter\n",
    "- From each 15 minute they gather data from smart meter and compute analytics\n",
    "- Consumption pattern of each user, hourly usage histogram. Computing daily we can estimate hourly requirement of specific user\n",
    "- Thermal sensitivity, out door temperature vs energy consumption.\n",
    "* Consumption prediction (daily trend) regression algo\n",
    "* Consumer grouping\n",
    "    - similar customer group and start campaign for energy saving for consumer groups.\n",
    "\n",
    "### Gaming\n",
    "* Data sources : using finger, Device type, VR headset, joy stick , mouse, what happen in games\n",
    "* Volume of data how many user, how much data we are tracking, type of game\n",
    "* Variety of data : Wheel data, track speed, touch data\n",
    "* What data to store? What is end goal?\n",
    "\n",
    "### Data model\n",
    "* Data model describes data characteristics. Structure of data, operations on data, constraint on data.\n",
    "* Structured data: Repeated pattern of data organization. Ex. csv, relational database. each line has same fields\n",
    "* Unstructured data : compress data like mp3, jpeg, mp4\n",
    "\n",
    "### Data operations\n",
    "* subsetting/selection/filtering\n",
    "* substructure extraction/ projection\n",
    "* Union / Union ALL\n",
    "* JOIN\n",
    "\n",
    "### Constraints\n",
    "* Value constraint : logical statement about data value. age can not -ve\n",
    "* Uniqueness constraint\n",
    "* cardinality constraint\n",
    "* Type constraint : type of data allowed\n",
    "* Domain constraint : allowed possible value day 1 to 31, month 1 to 12\n",
    "* Structural constraint : restrict structure of data instead data value it self. Ex. # rows = # of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational data model\n",
    "* Atomic : represent 1 unit of info and can not decompose further\n",
    "* No duplicates\n",
    "\n",
    "### Semi-structured data model\n",
    "* tree structure data\n",
    "* JSON XML\n",
    "* Data comes with tags. block s nested within larger block. but different document has different number of tags.\n",
    "* XML allows the querying of both schema and data. Ex. What is the name of element which contains sub element whose value is `temp`.  No such operation in relational database.\n",
    "* JSON has key value pairs\n",
    "\n",
    "### Vector space model\n",
    "* Useful for data model to retrieve text and images.\n",
    "* To find text we need a document vector model.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use Apache lucene to full text index and searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph data model\n",
    "* Social network is application \n",
    "* Data + connectivity, It has property of entities and edges. Also it has connectivity structure.\n",
    "![](images/property_graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Important use case is traversing based on some condition\n",
    "    - There are links like lives in, likes, Friend. Find restaurant which are liked by my friends living in NY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array as Data model\n",
    "* Images can be stored as array, each cell is tuple of (R,G,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data model vs Data Format\n",
    "* Data format is serialized representation of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Stream\n",
    "* Possibly unbounded sequence of data records. Each data is time stamped and may be geo-tagged.\n",
    "* Need to process data as it is generated\n",
    "* Constant stream of data from sensor or from social media\n",
    "* Sales trends, data driven marketing, Sales distribution, Monitoring and fault detection.\n",
    "* Amazon kinesis, Storm, Flink, samza, spark streaming\n",
    "* Data at rest: Mostly static from one or more sources. Data collected prior to analysis\n",
    "* Data in motion: Data analyzed as it is generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lambda.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Lakes\n",
    "* Many stream store data here in its original form. Data is stored in raw format.\n",
    "* During read add model(structure) to the data. Schema on read\n",
    "* We store all data for potential unknown use for later time.\n",
    "* In traditional data warehouse, data is loaded after transforming and making structured. Schema on write.\n",
    "* Data lakes store data as flat file with unique id.\n",
    "![](images/data_lakes.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each data is stored as Binary large object (BLOB) and has unique id. Each BLOB is tagged with number of metadata. So using metadata we can search data which we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data - Files vs DBMS:\n",
    "* In old times we used to store data in file system, Data redundancy, inconsistency and isolation (Hard to find data). Each task is program. Data integrity (Constraint) is hard coded. Atomicity of updates not available when data residing in more than one files.\n",
    "* DBMS offer declarative query language (We do not have to tell how to accomplish). No more task based programs\n",
    "* Data independence : Application dont worry about data storage format and location\n",
    "* Efficient access through optimization\n",
    "* Data integrity and security: ACID property of transaction, failure recovery.\n",
    "* Transfer money from one account to other is single transaction - Atomicity\n",
    "* Consistency means any data return to db must be valid according to defined rule\n",
    "* Concurrent access, multiple user can simultaneously access data without conflict.\n",
    "* Parallel database system\n",
    "    * Improve performance through parallel implementation\n",
    "    * Often allows data replication. Data redundancy more concurrent queries\n",
    "* Distributed Database system\n",
    "    * Data is stored across several sites, each site managed by a DBMS capable of running independently.\n",
    "* DBMS and MapReduce Style system\n",
    "    * Data stored in different partition and in parallel query can access different partition. Which can reduce data transfer time. \n",
    "    * Classic DBMS does not support machine failure.\n",
    "* DBMS take too much time to load data in database. DBMS offer too much functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDMS Big Data Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A flexible, semistructured data model\n",
    "    - Schema first to schema never\n",
    "* Support for new data like textual, temporal(time component before, after etc.) and spatial \n",
    "* A full query language.\n",
    "* Efficient parallel query engine\n",
    "* Continuous data ingestion, stream ingestion.\n",
    "* Scale gracefully to manage large volume\n",
    "\n",
    "#### ACID and BASE\n",
    "* Basic Availability : There will be a response\n",
    "* Soft State : State of system likely to change over time\n",
    "* Eventual Consistency : I upload a photo and my friend in India may see it, but other friend in china may see it later\n",
    "\n",
    "#### CAP theorem\n",
    "* A distributed computer can not simultaneously provide consistency, availability and partition tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AeroSpike\n",
    "* Distributed NoSQL db and key value store.\n",
    "* Standard scalar, lists, maps, geospatial, large object data type\n",
    "* Provide AQL an SQL like language.\n",
    "* Geospatial queries like point in polygon\n",
    "* Aerospike ensures ACID:\n",
    "    - Consistency: All copies of a data item are in sync. Use synchronous write to replica. Write is successful only when update to all replica is completed. No write allowed if previous write is pending. We can relax consistency if needed\n",
    "    - Durability: Flash storage. Replication management\n",
    "\n",
    "### AsterixDB\n",
    "* DBMS for Semi structure data. Provide ACID guarantee\n",
    "![](images/asterixDB.PNG)\n",
    "* Dataverse is namespace for data\n",
    "* Data is declared in terms of datatype. Blue user:TwitterUserType capture hierarchical structure of JSON.\n",
    "* open type meaning actual data can have more attribute than mentioned\n",
    "* close type meaning data must have same attribute. `?` meaning attribute is optional\n",
    "* AQL is a natively supported query language.\n",
    "* It can run over cluster. Hyracks is query execution engine which used for partitioned parallel execution of queries. Data is partitioned by hash partitioning and range partitioning.\n",
    "\n",
    "### Solr\n",
    "* Search text and indexing engine.\n",
    "* Text string can vary in numerous way. Spelling variation and capitalization, internal punctuation, full name vs first name, abbreviation, synonyms, initialism\n",
    "* Inverted Index: All terms in a collection of documents, In which document word appear and other info\n",
    "* With Solr we can have indexed fields of any structural document.\n",
    "* Faceted Search: Faceted search is a technique which involves augmenting traditional search techniques with a faceted navigation system, allowing users to narrow down search results by applying multiple filters based on faceted classification of the items.\n",
    "* Term highlighting: \n",
    "* Provides Tokenizers and filters, Specify how to parse document. Tokenization is process of breaking down the text. Filter out punctuation, tern to lower case, synonyms, remove common words.\n",
    "* StandardFilterFactory, LowerCaseFilterFactory, CommonGramsFilterFactory, StopFilterFactory, HTMLStripCharFilterFactory, PorterStemFilterFactory.\n",
    "\n",
    "### Vertica\n",
    "* Relational DBMS, design to operate on HDFS.\n",
    "* Column store architecture.\n",
    "![](images/row_col.PNG)\n",
    "* In row oriented design data is stored row by row. In column store store data column wise, Query only used columns needed, For large data it is faster.\n",
    "* Space efficiency, Column stores keep columns in sorted order. If next 20 entry is same then we can write entry and 20 besides it. Known as run length encoding\n",
    "* Frame of reference encoding: Fix a number and only record the difference.\n",
    "* Column-Groups: Frequently co-accessed columns behave as mini row stores within the column store. Use it to model data when we know that this set of column are accessed together\n",
    "* Update performance slower: When row is added, vertica initially places as row structure and then convert it to column structure and compress it. For large updates it is slow.\n",
    "* Support window operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big data processing\n",
    "* System should support split volumes of data streaming in\n",
    "* Access data fast\n",
    "* Distributed computing to node\n",
    "* Replication, scalability, scaling out and recovery\n",
    "* Optimized and extensible for many data types.\n",
    "* Streaming and batch processing\n",
    "    - Low latency (difference between processing time and event time/creation time) processing of streaming data\n",
    "    \n",
    "### Data retrieval\n",
    "* Way in which the desired data is specified and fetched.\n",
    "* Query language is a language to specify the data items you need.\n",
    "* It is declarative meaning specify what you need NOT how. Ex. SQL\n",
    "* Database programming language are procedural language which embed query operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Large table can be partitioned. \n",
    "* Range partitioning on primary key. Starts with A and B on machine 1. C and D on machine 2.\n",
    "* If query condition (WHERE) is applied on partitioning element, database know which machine has data. If query condition is different then we have to scan all the machines. Such query will be broadcasted from primary machine to all machine. Gather partial result and UNION.\n",
    "* To overcome searching all machines, we can use Index structure to identify where our data is. Given value, return records.\n",
    "* We can use local index on each machine\n",
    "* We can use machine index for each value (On which machine the data is for given value)\n",
    "![](images/indexing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join in distributed setting\n",
    "* Let's say we have schema\n",
    "\n",
    "```\n",
    "Frequents(drinker, bar)\n",
    "Likes(drinker, beer)\n",
    "```\n",
    "* Both table is stored in different machines\n",
    "* Query is \n",
    "\n",
    "```\n",
    "SELECT DISTINCT beer\n",
    "FROM Likes L, Frequents F\n",
    "WHERE bar = 'The Great American Bar' AND F.drinker = L.drinker\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On single machine system it is executed as\n",
    "![](images/single.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If Both tables are in different machine.\n",
    "* **Semijoin** fro mR to S on attribute is used to reduce data transmission cost.\n",
    "    - Project R on attribute A call R[A], here A is drinker.\n",
    "    - Ship this projection from site of R to site of S\n",
    "    - Reduce S to S' by eliminating tuples where attribute A are not matching any value in R[A].\n",
    "    \n",
    "![](images/semijoin1.jpg)\n",
    "![](images/semijoin2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map\n",
    "* Apply same operation to each member of a collection.\n",
    "* Discount each product by 5%\n",
    "\n",
    "#### Reduce\n",
    "* Collecting things that have same key.\n",
    "\n",
    "#### Cross/Cartesian\n",
    "* Multiplication\n",
    "* Do some process to each pair from 2 sets.\n",
    "\n",
    "#### Match/Join\n",
    "* Selective multiplication\n",
    "\n",
    "#### Group\n",
    "* Group common items, apply process to each group.\n",
    "\n",
    "#### Filter\n",
    "* Select elements that match a criteria\n",
    "\n",
    "#### Aggregation\n",
    "* Sum\n",
    "* Count\n",
    "* GROUP BY\n",
    "* Average\n",
    "* Max\n",
    "* Min\n",
    "* STD\n",
    "\n",
    "#### Analytical Operations\n",
    "* Classification\n",
    "* Clustering\n",
    "* Regression\n",
    "* Connectivity analysis\n",
    "* Associative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Execution model can be batch or streaming\n",
    "* Latency and scalability is also important\n",
    "* Programming language and how fault tolerance is handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hadoop provide Batch processing using disk storage. No in memory support. High latency. Java. Fault tolerance using replication of data\n",
    "* Spark batch and stream processing using disk or memory storage. Low latency for small micro batch size, Scala Python Java R\n",
    "* Fling: Batch and stream processing using disk or memory. Low latency, Java and Scala\n",
    "* Beam: Batch and stream processing. Low latency\n",
    "* Storm: Stream processing, very low latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "* In hadoop we have to make our task with map and reduce only. It is not always possible, we need sampling, filter, join.\n",
    "* MR relies on reading from HDFS\n",
    "* No interactive shell support in Hadoop\n",
    "* No support for streaming in Hadoop\n",
    "* Only Java in Hadoop\n",
    "* Spark has expressive programming model. In memory processing. Interactive shell.\n",
    "* RDD is built from data from HDFS, S3, HBase,JSON, text, local files\n",
    "* Driver program has Spark Context, worker program has Spark Executors. Driver program creates RDD.\n",
    "* Narrow transformation: Data shuffling not needed. Transformation only on data residing in partition. Ex flat, flatMap. All computation done in worker node, without transfer of data.\n",
    "* Coalesce: Reduce the number of Partition. When using filter we get rid of data and partition size is small, use coalesce to combine partitions.\n",
    "* Wide transformation : groupByKey, reduceByKey. Requires shuffling of data across the worker node\n",
    "* collect, ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration\n",
    "* When merger happen, parent company try to merge all dataset of its own and new merged company. We need some software solution that can behaves like all this data is unified. Such system is called information integration system.\n",
    "* Schema mapping is needed to combine 2 data sources.\n",
    "* Pay as you go model: Only integrate sources that are needed when needed.\n",
    "* Attribute grouping: How similar are the attributes, how likely that 2 attribute would co-occur?\n",
    "* Data fusion: Finding value of data items from source. Also finding distribution of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
