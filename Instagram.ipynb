{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Flickr, Picasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instagram is a social networking service which enables its users to upload and share their photos and videos with other users. Instagram users can choose to share information either publicly or privately. Anything shared publicly can be seen by any other user, whereas privately shared content can only be accessed by a specified set of people. Instagram also enables its users to share through many other social networking platforms, such as Facebook, Twitter, Flickr, and Tumblr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* simpler version of Instagram, where a user can share photos and can also follow other users. The ‘News Feed’ for each user will consist of top photos of all the people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement\n",
    "* Users should be able to upload/download/view photos.\n",
    "* Users can perform searches based on photo/video titles.\n",
    "* Uers can follow other users.\n",
    "* The system should be able to generate and display a user’s News Feed consisting of top photos from all the people the user follows.\n",
    "* Like and comment on image. Does comment is recursive? Instagram only allows one layer of recursion\n",
    "\n",
    "### Non functional\n",
    "* Our service needs to be highly available.\n",
    "* The acceptable latency of the system is 200ms for News Feed generation.\n",
    "* Consistency can take a hit (in the interest of availability), if a user doesn’t see a photo for a while; it should be fine.\n",
    "* The system should be highly reliable; any uploaded photo or video should never be lost.\n",
    "\n",
    "### Extended\n",
    "* Adding tags to photos, searching photos on tags, commenting on photos, tagging users to photos, who to follow, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "* The system would be read-heavy, so we will focus on building a system that can retrieve photos quickly.\n",
    "* Practically, users can upload as many photos as they like. Efficient management of storage should be a crucial factor while designing this system.\n",
    "* Low latency is expected while viewing photos.\n",
    "* Data should be 100% reliable. If a user uploads a photo, the system will guarantee that it will never be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity\n",
    "* 500 M total users and 1M daily active users\n",
    "* 2M new photos every day. 23 new photo every second\n",
    "* Photo size 200KB\n",
    "* 1 day photo space = 2M * 200KB = 400 GB\n",
    "* 10 years space = 400 * 365 * 10 = 1425TB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* upload photos and the other to view/search photos. Our service would need some object storage servers to store photos and also some database servers to store metadata information about the photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to store data about users, their uploaded photos, and people they follow. Photo table will store all data related to a photo; we need to have an index on (PhotoID, CreationDate) since we need to fetch recent photos first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can store photos in a distributed file storage like HDFS or S3.\n",
    "* We can store the above schema in a distributed key-value store to enjoy the benefits offered by NoSQL. All the metadata related to photos can go to a table where the ‘key’ would be the ‘PhotoID’ and the ‘value’ would be an object containing PhotoLocation, UserLocation, CreationTimestamp, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to store relationships between users and photos, to know who owns which photo. We also need to store the list of people a user follows. For both of these tables, we can use a wide-column datastore like Cassandra. For the ‘UserPhoto’ table, the ‘key’ would be ‘UserID’ and the ‘value’ would be the list of ‘PhotoIDs’ the user owns, stored in different columns. We will have a similar scheme for the ‘UserFollow’ table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cassandra or key-value stores in general, always maintain a certain number of replicas to offer reliability. Also, in such data stores, deletes don’t get applied instantly, data is retained for certain days (to support undeleting) before getting removed from the system permanently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming each “int” and “dateTime” is four bytes, each row in the User’s table will be of 68 bytes\n",
    "     - UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we have 500 million users, we will need 32GB of total storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Photo: Each row in Photo’s table will be of 284 bytes\n",
    "    - PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes\n",
    "* If 2M new photos get uploaded every day, we will need 0.5GB of storage for one day\n",
    "* For 10 years we will need 1.88TB of storage.\n",
    "* If we have 500 million users and on average each user follows 500 users. We would need 1.82TB of storage for the UserFollow table:\n",
    "    - 500 million users * 500 followers * 8 bytes ~= 1.82TB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Photo uploads (or writes) can be slow as they have to go to the disk, whereas reads will be faster, especially if they are being served from cache.\n",
    "* Uploading users can consume all the available connections, as uploading is a slow process. This means that ‘reads’ cannot be served if the system gets busy with all the write requests. We should keep in mind that web servers have a connection limit before designing our system. If we assume that a web server can have a maximum of 500 connections at any time, then it can’t have more than 500 concurrent uploads or reads. To handle this bottleneck we can split reads and writes into separate services. We will have dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the system.\n",
    "* Separating photos’ read and write requests will also allow us to scale and optimize each of these operations independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability and Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Losing files is not an option for our service. Therefore, we will store multiple copies of each file so that if one storage server dies we can retrieve the photo from the other copy present on a different storage server.\n",
    "* If we want to have high availability of the system, we need to have multiple replicas of services running in the system, so that if a few services die down the system still remains available and running. Redundancy removes the single point of failure in the system.\n",
    "* If only one instance of a service is required to run at any point, we can run a redundant secondary copy of the service that is not serving any traffic, but it can take control after the failover when primary has a problem.\n",
    "* if there are two instances of the same service running in production and one fails or degrades, the system can failover to the healthy copy. Failover can happen automatically or require manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let’s discuss different schemes for metadata sharding:\n",
    "* Partitioning based on UserID Let’s assume we shard based on the ‘UserID’ so that we can keep all photos of a user on the same shard. If one DB shard is 1TB, we will need four shards to store 3.7TB of data. Let’s assume for better performance and scalability we keep 10 shards.\n",
    "\n",
    "* So we’ll find the shard number by UserID % 10 and then store the data there. To uniquely identify any photo in our system, we can append shard number with each PhotoID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How can we generate PhotoIDs? Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problems\n",
    "    - How to handle hot users? Several people follow such hot users and a lot of other people see any photo they upload.\n",
    "    - Some users will have a lot of photos compared to others, thus making a non-uniform distribution of storage.\n",
    "    - What if we cannot store all pictures of a user on one shard? If we distribute photos of a user onto multiple shards will it cause higher latencies?\n",
    "    - Storing all photos of a user on one shard can cause issues like unavailability of all of the user’s data if that shard is down or higher latency if it is serving high load etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Partitioning based on PhotoID If we can generate unique PhotoIDs first and then find a shard number through “PhotoID % 10”, the above problems will have been solved. We would not need to append ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How can we generate PhotoIDs? Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored. One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs. If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field. So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo.\n",
    "* Wouldn’t this key generating DB be a single point of failure? Yes, it would be. A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered. \n",
    "* We can put a load balancer in front of both of these databases to round robin between them and to deal with downtime. Both these servers could be out of sync with one generating more keys than the other, but this will not cause any issue in our system. We can extend this design by defining separate ID tables for Users, Photo-Comments, or other objects present in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can implement a ‘key’ generation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can have a large number of logical partitions to accommodate future data growth, such that in the beginning, multiple logical partitions reside on a single physical database server. Since each database server can have multiple database instances on it, we can have separate databases for each logical partition on any server. So whenever we feel that a particular database server has a lot of data, we can migrate some logical partitions from it to another server. We can maintain a config file (or a separate database) that can map our logical partitions to database servers; this will enable us to move partitions around easily. Whenever we want to move a partition, we only have to update the config file to announce the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking and News Feed Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To create the News Feed for any given user, we need to fetch the latest, most popular and relevant photos of the people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For simplicity, let’s assume we need to fetch top 100 photos for a user’s News Feed. Our application server will first get a list of people the user follows and then fetch metadata info of latest 100 photos from each user. In the final step, the server will submit all these photos to our ranking algorithm which will determine the top 100 photos (based on recency, likeness, etc.) and return them to the user. A possible problem with this approach would be higher latency as we have to query multiple tables and perform sorting/merging/ranking on the results. To improve the efficiency, we can pre-generate the News Feed and store it in a separate table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can have dedicated servers that are continuously generating users’ News Feeds and storing them in a ‘UserNewsFeed’ table. So whenever any user needs the latest photos for their News Feed, we will simply query this table and return the results to the user.\n",
    "\n",
    "* Whenever these servers need to generate the News Feed of a user, they will first query the UserNewsFeed table to find the last time the News Feed was generated for that user. Then, new News Feed data will be generated from that time onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pull: Clients can pull the News Feed contents from the server on a regular basis or manually whenever they need it. Possible problems with this approach are a) New data might not be shown to the users until clients issue a pull request b) Most of the time pull requests will result in an empty response if there is no new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Push: Servers can push new data to the users as soon as it is available. To efficiently manage this, users have to maintain a Long Poll request with the server for receiving the updates. A possible problem with this approach is, a user who follows a lot of people or a celebrity user who has millions of followers; in this case, the server has to push updates quite frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With long polling, the client requests information from the server exactly as in normal polling, but with the expectation the server may not respond immediately. If the server has no new information for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available. Once it does have new information, the server immediately sends an HTTP/S response to the client, completing the open HTTP/S Request. Upon receipt of the server response, the client often immediately issues another server request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hybrid: We can adopt a hybrid approach. We can move all the users who have a high number of follows to a pull-based model and only push data to those users who have a few hundred (or thousand) follows. Another approach could be that the server pushes updates to all the users not more than a certain frequency, letting users with a lot of follows/updates to regularly pull data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Feed Creation with Sharded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the most important requirement to create the News Feed for any given user is to fetch the latest photos from all people the user follows. For this, we need to have a mechanism to sort photos on their time of creation. To efficiently do this, we can make photo creation time part of the PhotoID. As we will have a primary index on PhotoID, it will be quite quick to find the latest PhotoIDs.\n",
    "\n",
    "* We can use epoch time for this. Let’s say our PhotoID will have two parts; the first part will be representing epoch time and the second part will be an auto-incrementing sequence. So to make a new PhotoID, we can take the current epoch time and append an auto-incrementing ID from our key-generating DB. We can figure out shard number from this PhotoID ( PhotoID % 10) and store the photo there.\n",
    "\n",
    "* What could be the size of our PhotoID? Let’s say our epoch time starts today, how many bits we would need to store the number of seconds for next 50 years?\n",
    "\n",
    "    - 86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds\n",
    "* We would need 31 bits to store this number. Since on the average, we are expecting 23 new photos per second; we can allocate 9 bits to store auto incremented sequence. So every second we can store (2^9 => 512) new photos. We can reset our auto incrementing sequence every second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache and Load balancing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our service would need a massive-scale photo delivery system to serve the globally distributed users. Our service \n",
    "should push its content closer to the user using a large number of geographically distributed photo cache servers and use CDNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can introduce a cache for metadata servers to cache hot database rows. We can use Memcache to cache the data and Application servers before hitting database can quickly check if the cache has desired rows. Least Recently Used (LRU) can be a reasonable cache eviction policy for our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we go with 80-20 rule, i.e., 20% of daily read volume for photos is generating 80% of traffic which means that certain photos are so popular that the majority of people read them. This dictates that we can try caching 20% of daily read volume of photos and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram\n",
    "* SSL terminates at the ELB, which lessens the CPU load on nginx.\n",
    "* Twelve PostgreSQL replicas run in a different availability zone.\n",
    "* Amazon CloudFront as the CDN.\n",
    "* Redis powers their main feed, activity feed, sessions system. Redis runs in a master-replica setup. Replicas constantly save to disk.\n",
    "* Apache Solr powers the geo-search API\n",
    "* Gearman (Gearman provides a generic application framework to farm out work to other machines or processes that are better suited to do the work. It allows you to do work in parallel, to load balance processing, and to call functions between languages. It can be used in a variety of applications, from high-availability web sites to the transport of database replication events. In other words, it is the nervous system for how distributed processing communicates.) is used to: asynchronously share photos to Twitter, Facebook, etc; notifying real-time subscribers of a new photo posted; feed fan-out. 200 Python workers consume tasks off the Gearman task queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Everyday :\n",
    "    - 400 Million users visits\n",
    "    - 4 Billion likes\n",
    "    - 100 million photo/video upload\n",
    "    - 110 million followers for top account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instagram stack\n",
    "* Web tier in python Django, receive request and access other services for response.\n",
    "* Some user request are handled asynchronously in backend (Liking someone photo and sending notification to the person), push it to queue like RebbitMQ\n",
    "* Storage vs Computing\n",
    "    - Storage needs to be consistent across data centers with replication (perhaps with latency but eventual consistent)\n",
    "    - Computing driven by user traffic as needed basis (Stateless, data in servers are temporary and can reconstructed from global data)\n",
    "* PostgresSQL : User, media, friendship etc.\n",
    "    - Deployed in every region as master and multiple replica.\n",
    "    - Django write to masters of multiple region, but read is conducted only from local region.\n",
    "![](images/instagram7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cassandra:\n",
    "    - User feeds, activities\n",
    "    - There is no master, all replica has same copy of data with eventual consistency\n",
    "    - Consistency can be configured as per application tolerance level, some can set to write consistency of 2, and read to 1.\n",
    "    - We can have repica living in different data centers\n",
    "![](images/instagram8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For computing\n",
    "    - Combine Django, RabbitMQ and celery (Celery is an open source asynchronous task queue or job queue which is based on distributed message passing.) in one pod that goes in each region, global load balancer will balance the user request to Django. Asynchronous task is produced and consume in the same region.\n",
    "![](images/instagram9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In image, database replication across the region and computing resources contained within the region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memcache\n",
    "    - High performance key-val store in memory\n",
    "    - Millions of reads/write per second\n",
    "    - Sensitive to network condition because of large number of request and so latency can be affected, so cross region read/write is prohibited.\n",
    "    - No global consistent memcache\n",
    "    - Memcache in each region is determined by user traffic served out of that region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In same region, user C make comment saved in both db and cache, in same region user R wants feed , comment can be read from memcache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram11.png)\n",
    "* Other user connected to different DC, DC2\n",
    "* From DC1 comment gets replicated to db of DC2. But cache of DC2 is still the stale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead Django update a cache, make db to update cache\n",
    "![](images/instagram12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It can create a load on postgres server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a one image with 1.2 million likes\n",
    "\n",
    "```\n",
    "SELECT COUNT(*) form user_likes_media WHERE media_id = 1234;\n",
    "```\n",
    "* Takes 100 ms\n",
    "* Before when we used to write in cache directly it was the issue of incrementing a counter in memcache which is very fast\n",
    "* But now after every like postgres invalidate the cache and calculate new likes and update the cache.\n",
    "* Denormalize the db, which stores mediaid and number of likes,\n",
    "```\n",
    "SELECT count from media_like where media_id = 1234;\n",
    "```\n",
    "* When cache is invalidated all the read likes request go to db, which increases the traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memcache lease\n",
    "![](images/instagram13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First django server try to read from memcache as lease-get, memcache will permit it to go to db, meanwhile if second request comes to memcache, it will not allow to go to db, but suggested to use stale data. After first server fetch the value from db, it will do lease-set to memcache, after that it updates the cache. \n",
    "* So, leases work in practice because only one client is headed to the DB at any given time for a particular value in the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when a new delete comes in, memcached will know that the next lease-set (the one from client A) is already stale, so it will accept the value (because it's newer) but it will mark it as stale and the next client to ask for that value will get a new lease token, and clients after that will once again get hot miss results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use few CPU instructions as possible\n",
    "    - Writing good code\n",
    "* Use few server as possible to handle request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use profiling tool to see which part of code is taking high time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When client request feed, server return the URL of CDN where client can find the media.\n",
    "* URL can be generated using URL generator,\n",
    "* URL is generated depends on where the user is located, what kind of network it is connected with, what kind of device is there\n",
    "    - To generae different URL for different media size they used to call URL generator multiple times.\n",
    "    - Instead call once and overwrite the size function.\n",
    "* Function which are stable and used extensively develop them in C/C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To get more from single server, run multiple instance of worker process in the server. But it is upper bounded by server memory. There is private memory for each worker and there is a shared memory.\n",
    "* How to reduce memory requirement to run more processes\n",
    "    - Reduce the code in memory\n",
    "    - Remove dead code (Cprofile)\n",
    "    - Provide restrictive private memory\n",
    "    - move configuration into shared memory (treadoff of latency)\n",
    "    - Disable garbage collection (Python requires GC in private part only)\n",
    "* Each process can handle one request at a time so while it is waiting for external service wastage of resources.\n",
    "    - Home feed has stories, posts. Which can increase latency for retrieval.\n",
    "    - Instead of requesting suequentially to feed, stories and suggested users services, we can have asynchronous IO to access simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which server?\n",
    "* New table or new column?\n",
    "* What index?\n",
    "* Should i cache it?\n",
    "* Will I lock up DB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tao\n",
    "    - Database plus write through cache, Used RDBMS as a backend to store data, but very simplified model. Uss nodes as an object and edges with relationship.\n",
    "    - We can not make direct sophosticated queries in db itself.\n",
    "![](images/instagram14.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load test:\n",
    "    - Artificial load, that are semi triggered by user request. how many users will be using that features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Every request to Instagram servers goes through load balancing machines; we used to run 2 nginx machines and DNS Round-Robin between them. The downside of this approach is the time it takes for DNS to update in case one of the machines needs to get decomissioned. Recently, we moved to using Amazon’s Elastic Load Balancer, with 3 NGINX instances behind it that can be swapped in and out (and are automatically taken out of rotation if they fail a health check). We also terminate our SSL at the ELB level, which lessens the CPU load on nginx. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The photos themselves go straight to Amazon S3, which currently stores several terabytes of photo data for us. We use Amazon CloudFront as our CDN, which helps with image load times from users around the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a user decides to share out an Instagram photo to Twitter or Facebook, or when we need to notify one of our Real-time subscribers of a new photo posted, we push that task into Gearman, a task queue system originally written at Danga. Doing it asynchronously through the task queue means that media uploads can finish quickly, while the ‘heavy lifting’ can run in the background. We have about 200 workers (all written in Python) consuming the task queue at any given time, split between the services we share to. We also do our feed fan-out in Gearman, so posting is as responsive for a new user as it is for a user with many followers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* how to assign unique identifiers to each piece of data in the database (for example, each photo posted in our system). The typical solution that works for a single database — just using a database’s natural auto-incrementing primary key feature — no longer works when data is being inserted into many databases at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generated IDs should be sortable by time\n",
    "* IDs should ideally be 64 bits (for smaller indexes, and better storage in systems like Redis\n",
    "* Generate IDs in web application\n",
    "    - This approach leaves ID generation entirely up to your application, and not up to the database at all. For example, MongoDB’s ObjectId, which is 12 bytes long and encodes the timestamp as the first component. Another popular approach is to use UUIDs.\n",
    "    - Each application thread generates IDs independently, minimizing points of failure and contention for ID generation\n",
    "    - If you use a timestamp as the first component of the ID, the IDs remain time-sortable\n",
    "    - Generally requires more storage space (96 bits or higher) to make reasonable uniqueness guarantees\n",
    "    - Some UUID types are completely random and have no natural sort\n",
    "* Generate IDs through dedicated service\n",
    "    - Twitter’s Snowflake, a Thrift service that uses Apache ZooKeeper to coordinate nodes and then generates 64-bit unique IDs\n",
    "    - Snowflake IDs are 64-bits, half the size of a UUID\n",
    "    - Can use time as first component and remain sortable\n",
    "    - Distributed system that can survive nodes dying\n",
    "    - Would introduce additional complexity and more ‘moving parts’ (ZooKeeper, Snowflake servers) into our architecture\n",
    "* DB Ticket Servers\n",
    "    - Uses the database’s auto-incrementing abilities to enforce uniqueness. Flickr uses this approach, but with two ticket DBs (one on odd numbers, the other on even) to avoid a single point of failure.\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Statelessness means they can bounce people around servers and it's easier to make their APIs.\n",
    "* Scaled at first by replication, but that only helps with reads.\n",
    "* Create a search farm by replicating the portion of the database they want to search.\n",
    "* - Shards: My data gets stored on my shard, but the record of performing action on your comment, is on your shard. When making a comment on someone else's’ blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clicking a Favorite:\n",
    "    - Pulls the Photo owners Account from Cache, to get the shard location (say on shard-5)\n",
    "    - Pulls my Information from cache, to get my shard location (say on shard-13)\n",
    "    - Starts a “distributed transaction” - to answer the question: Who favorited the photo? What are my favorites?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each server in shard is 50% loaded. Shut down 1/2 the servers in each shard. So 1 server in the shard can take the full load if a server of that shard is down or in maintenance mode. To upgrade you just have to shut down half the shard, upgrade that half, and then repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Average queries per page, are 27-35 SQL statements. Favorites counts are real time. API access to the database is all real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A lot of data is stored twice. For example, a comment is part of the relation between the commentor and the commentee. Where is the comment stored? How about both places? Transactions are used to prevent out of sync data: open transaction 1, write commands, open transaction 2, write commands, commit 1st transaction if all is well, commit 2nd transaction if 1st committed. but there still a chance for failure when a box goes down during the 1st commit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Photos are stored on the filer. Upon upload, it processes the photos, gives you different sizes, then its complete. Metadata and points to the filers, are stored in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tags do not fit well with traditional normalized RDBMs schema design. Denormalization or heavy caching is the only way to generate a tag cloud in milliseconds for hundreds of millions of tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of their data views are calculated offline by dedicated processing clusters which save the results into MySQL because some relationships are so complicated to calculate it would absorb all the database CPU cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical URL for a Flickr image looks like this:\n",
    "\n",
    "http://farm1.static.flickr.com/104/301293250_dc284905d0_m.jpg\n",
    "\n",
    "If we split this up we get:\n",
    "\n",
    "farm1 - Obviously the farm at which the image is stored. I have yet to see a value other than one.\n",
    "\n",
    ".static.flickr.com - Fairly self explanitory.\n",
    "\n",
    "/104 - The server ID number.\n",
    "\n",
    "/301293250 - The image ID.\n",
    "\n",
    "_dc284905d0 - The image 'secret'. I assume this is to prevent images being copied without first getting the information from the API.\n",
    "\n",
    "_m - The size of the image. In this case the 'm' denotes medium, but this can be small, thumb etc. For the standard image size there is no size of this form in the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/instagram3.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
