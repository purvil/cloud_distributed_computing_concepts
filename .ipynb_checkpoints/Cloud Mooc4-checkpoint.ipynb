{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Data analytics Tools\n",
    "\n",
    "### Hortonworks\n",
    "\n",
    "#### HDP(Hortonworks Data platform)\n",
    "![](images/HDP.jpg)\n",
    "* Zepplin : Web based notebook that enables interactive data analytics. Takes lots of different language.\n",
    "* Ambari : Source management platform for provisioning, managing, monitoring and securing Apache Hadoop clusters.\n",
    "* Tez: Helps to understand and optimize cluster resource usage. Optimize or accelerate individual SQL or Pig jobs.\n",
    "* Hive: Allows to write and execute SQL queries on the cluster. Show the history of all past jobs. Allows to see graphical view of execution plan.\n",
    "* Pig: Allows writing and running pig scripts. Extract-transform-load data pipelines\n",
    "    - Extensible: Can add custom function to meet their processing requirement\n",
    "    - Easily programmed: Complex task can be represented as data flow sequences\n",
    "    - Self optimizing \n",
    "* Capacity scheduler: allows YARN workload management\n",
    "* Files: ALlows the user to manage, browse and upload files and folders in HDFS\n",
    "* Solr: Full text search and near real time indexing.\n",
    "* MapReduce : Batch processing framework for structure and unstructured data\n",
    "    - Simple we can write in any application\n",
    "    - Scalability: It can process peta bytes of data stored in HDFS\n",
    "    - Speed: Parallel processing\n",
    "    - Recovery: \n",
    "    - Minimal data motion: Data locality.\n",
    "#### Hortonworks data flow (HDF):\n",
    "* NiFi, Kafka, Storm provides real time dataflow management and streaming analytics\n",
    "* Enables real time data collection, curation, analysis and delivery of data to and form any source.\n",
    "\n",
    "### Cloudera\n",
    "![](images/CLoudera.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MapReduce framework provides\n",
    "    - UDF\n",
    "    - Automatic parallelization and distribution\n",
    "    - Fault tolerance\n",
    "    - I/O scheduling\n",
    "    - Status and monitoring\n",
    "* We used to have MPI architecture (Message passing Interface)\n",
    "* We have MPI_send and MPI_receive instructions\n",
    "* Deadlock may happen, blocking communication occurs dead lock. It has to managed by program\n",
    "* Large overhead for communication mismanagement\n",
    "* Load imbalance\n",
    "* Dead machine\n",
    "* To overcome above problems we have Distributed file system, which takes care of all theses. EX. GFS and HDFS\n",
    "* MR, makes easy to distribute. Takes care of parallelism. System takes care of balancing and dead machines, retry and failure support\n",
    "* Map perform a function on individual values in a dataset to create new list of values. Reduce combine values in a dataset to create new value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Pi Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Radius of 1 unit circle is pi.\n",
    "* If we draw square around circle area of circle is 4.\n",
    "* Imagine we have many dirt and shoot it on such board. Compute how many are inside the circle and how many are outside.\n",
    "* Mapper, generate points in unit square and then count point inside the circle and outside the circle using inscribe circle of the square. Will output (inside, 1) (total, 1)\n",
    "* Reducer accumulate points.\n",
    "* NumInside/ numTotal = pi\n",
    "\n",
    "### Image smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One approach is to use sliding mask and replace each value of pixel.\n",
    "* Average value of all neighbor 9 pixels and compute average, replace the value of middle pixel with the average.\n",
    "* Map has input key = x,y and value R,G,B\n",
    "* Map emit 9 points (x-1, y-1,R,G,B), (x,y-1,R,G,B)), (x+1, y-1,R,G,B) etc.\n",
    "* Reduce input key = x,y input value list of R,G,B, compute avg of R,G,B. EMit key (x,y) and value avg RGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eventual Consistency\n",
    "\n",
    "### CAP theorem, Brewer's theorem\n",
    "- It is impossible for a distributed system to simultaneously provide all of below 3 Explain what database is possible to implement, why there is no scalable relational database. Outlines limit of distributed system\n",
    "1. Consistency : All nodes see the same data at same time.  Most recent data or error.\n",
    "2. Availability : A guarantee that every request receives a response in case of success or failure. Every request received by a non failing node must result in a response.\n",
    "3. Partition Tolerance: System continue to operate despite arbitrary message loss or failure of part of system. No set of failure less than total network failure is allowed to cause the system to respond incorrectly.\n",
    "![](images/CAP.png)\n",
    "* Left user want to update key = 42, it is connected to server A, it update there and, update propagates to server B and right user then want to read key from server B and get value 42.\n",
    "* Now image network is partitioned between server A and server B,\n",
    "    - Server A will not get reply from B.\n",
    "    - It can be due to network issue, network misconfiguration, Software bugs, hardware issue, server B is too busy with its own task (CPU or IO contention) and does not handle request from server A.\n",
    "    - Now host A can return an error when left user wants to update key, meaning choosing consistency over availability.\n",
    "    - Other option is write data to server A and propagate to server B later. Choosing availability over consistency\n",
    "    - Partition issue occur rarely so in  general case it is trade of between low latency and consistency. Between write as fast as possible vs reading stale data.\n",
    "* SQL are CA. Most NoSQL data stores give a choice. write/read as fast as possible (AP) or write to more host (CP). Read from more host(CP). Most NoSQL provides per operation based configuration.\n",
    "* In asynchronous model we just send a message over the network, there is no ACK we just assume that our message has reached the destination.\n",
    "* In asynchronous model we can not guarantee both availability and atomic consistency. There is no clock, decision are made with local computer or messages.\n",
    "* In partially synchronous model, each node has a clock and all clock increase at the same rate. Can use clocks as timers to know whether received message.\n",
    "* There are partially synchronous algorithms that will return atomic data when all message in an execution are delivered (no partition) and will only return inconsistent data when message are lost.\n",
    "* t-connected consistency: If no partition then system is consistent, If partition then system can return stale data. Once partition heals within some window of time, system will return to consistency.\n",
    "* How much consistency needed?\n",
    "    - Youtube video, Number of units available in amazon (as long as count is not 0 customer will dont care if the count is off by some value.)\n",
    "* For such system we have several views\n",
    "    * Client sees a snapshot of the database that is internally consistent and might be valid.\n",
    "    * Internally, database is genuinely consistent but the states client saw are not tracked and might sometime become invalidated by an update\n",
    "    * Inconsistency is tolerated because it yields such big speedups, although some clients sees wrong results.\n",
    "* We have to think, how many updates will occur.\n",
    "* How often those updates conflict with concurrent reads or will concurrent updates.\n",
    "* In most large cloud system contention is rare so transactional database solution work. Use a relaxed consistency\n",
    "* Inconsistency can cause bug, client will never be able to trust servers.\n",
    "* Weak or best effort consistency?\n",
    "    - Strong security guarantees demand consistency\n",
    "    - Would you trust a medical electronic health records system or a bank that used weak consistency?\n",
    "* Consistency : Updates in an agreed order\n",
    "* Durability : Once updated. won't be forgotten\n",
    "* Real time responsiveness: Replies with bounded delay\n",
    "* Security : Only permits authorized action by authenticated parties\n",
    "* Privacy : Do not disclose personal data\n",
    "* Fault-tolerance : Failures can not prevent the system from providing desired service\n",
    "* Coordination: Action wont interfere with one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAP warns that strong properties can lead to slow services\n",
    "![](images/consistency.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ebay\n",
    "* 112 million of active users and 6 billion reads and 5 billion writes per day. 250 TB of data and need 100% uptime.\n",
    "\n",
    "#### Spotify\n",
    "* Online streaming service, store 1.5 billion playlists. 40000 request/second, latency as low as possible, 4000 database servers, needs 100% uptime\n",
    "\n",
    "* Relational database: supports frequent reads and write (Apache hive mostly work on immutable data). It supports power full operation like GROUP BY, JOIN and provides ACID transaction: execute all commands atomically or none\n",
    "* It can only work on single machine. It can only store data in table like model. Can not efficiently represent graphs. High latency for some case. Can not give 100% availability.\n",
    "* NOSQL database: horizontal scalable, stores larger amount of data, no single data model.  Limited or no transaction support, Eventual consistency (BASE)\n",
    "* Document storage : MongoDB, CounchDB, RethinkDB\n",
    "* Key-value storage : like distributed hash map, Redis, AWS DynamoDB, Aerospike, Cassandra\n",
    "* Columnar storage : Instead of storing table per file, it store column per file which allows to iterate on single column very fast. Google BigQuery, AWS redShift\n",
    "* Graph Databases: Neo4j, OrientDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To gain consistency\n",
    "* We can use Distributed transaction with locking, Zookeeper and Paxos use locking\n",
    "* 2-phase locking\n",
    "* Distributed transaction with time stamps, lamport's logical clocks\n",
    "* Distributed transaction with snapshots\n",
    "\n",
    "### ACID\n",
    "* Atomicity : Even if transaction have multiple operations, execute all of them or none\n",
    "* Consistency : A transaction run on correct database leaves it in a correct state\n",
    "* Isolation : It looks as if each transaction ran all by itself (Hide concurrency)\n",
    "* Durability : Once transaction commits, updates can not be lost or can not be rolled back\n",
    "* Transaction are basis of ACID\n",
    "    - Application are coded as begin transaction, do operation, commit or abort\n",
    "    - System executes in all or nothing way\n",
    "* No need to worry about partial state. \n",
    "* Transaction can not see partially completed step of other concurrent transaction.\n",
    "\n",
    "### BASE\n",
    "* Proposed by ebay.\n",
    "* Allow to cache, allow to use outdated stale data.\n",
    "* Involves step by step transformation of a transactional application into one that will be concurrent and less rigid. Does not guarantee ACID.\n",
    "* Basically Available Soft-state Services with Eventual consistency.\n",
    "*  Basically Available : Fast response even if some replica are slow or crashed\n",
    "* Soft-state service: No durable memory, can not store any permanent data, restart in clean state after crash. To remember make enough replicas or pass it to other service that keeps hard state\n",
    "* Eventual consistency : Ok to send optimistic answers to the external client. Cached data, guess outcome of update, might skip locks hoping that no conflicts will happen, If needed correct any inconsistencies in an offline cleanup activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ACID code often too slow, scale poorly and end user has to wait\n",
    "* With BASE code is concurrent so faster, Elimination of locking, early response, all make end user experience snappy and positive. Upload youtube video and search for it, we might not get it. Slightly different bidding history for different people in ebay, but it makes ebay 10x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need centralize service (Coordination kernel) Which maintains Config info, Naming, Distributed Synchronization, Group service. Which avoids synchronization and races\n",
    "\n",
    "### Paxos\n",
    "* The proposer request that Paxos system accept some command. Paxos is like postal system. Paxos thinks about letter for a while (Decide whether replication of data needed and determine order). Once these are decided, learner can execute the command. \n",
    "* Client issues a request to the distributed system, and waits for a response (Ex. Write request on a file in a distributed file server)\n",
    "* Acceptors act as the fault tolerant memory of the protocol. Acceptors are collected into groups called Quorums\n",
    "* Any message sent to an acceptor must be sent to a quorum of acceptors\n",
    "* Any message received from an Acceptor is ignored unless copy is received from each acceptor in Quorum.\n",
    "* A consensus algo can make progress using 2F + 1 processord despite the simultaneous failure of F processors.\n",
    "\n",
    "* Paxos has proposer, acceptors and learners. All communicates.\n",
    "* Proposer advocates a client request, attempting to convince the acceptors to agree on it and learners act as the replication factor for the protocol. Once client request has been agreed by acceptors, the learner may take action that is execute the request and send response to client. To improve availability there can be multiple learners.\n",
    "* Paxos requires a distinguishable proposer (called leader) to make progress. Many process may believe they are leaders, but the protocol only guarantees progress if one of them is eventually chosen.\n",
    "* If 2 processes believe they are leader they may stall the protocol by proposing conflicting updates. \n",
    "* Each proposal is uniquely numbered for a given proposer.\n",
    "* A proposer (leader) creates a proposal identified with number N.\n",
    "* Number must be more than any previous proposal number\n",
    "* Then it sends a prepare message containing this proposal to Quorum of acceptors. \n",
    "* If proposal's number N is higher than any previous proposal number then the acceptors must return promise to ignore all future proposals having number less than N.If acceptor accepted a proposal at some point in the past, it must include the previous proposal number and previous value in its response to the proposer.\n",
    "* Otherwise acceptor can ignore received proposal. However it may respond with denial so proposer can have knowledge of that.\n",
    "* If proposer receive enough promises from a quorum of acceptors, it needs to set a value to its proposal\n",
    "* If any acceptors had previously accepted by proposal, then they have sent value to proposer, who now must set the value of its proposal to the value associated with the highest proposal number reported by the acceptors.\n",
    "* If none of the acceptors had accepted a proposal up to this point, then the proposer may choose any value for its proposal.\n",
    "* The proposal sends an accept request message to a quorum of acceptors with the chosen value for its proposal.\n",
    "* If an acceptor receives an accept request message for a proposal N, it must accept it if and only if it has not already promised to only consider proposals having an identifier greater than N. In this can it should register the corresponding value v and send an accepted message to the proposer and every learner else ignore accept request.\n",
    "* Rounds failed when multiple proposers send conflicting prepare messages or when the proposer does not receive quorum of responses. In this case another round will be started with higher proposal number\n",
    "* Notice that when acceptors accept a request, they also ack the leadership of the proposer. Hence, Paxos can be used to select a leader in a cluster of nodes.\n",
    "\n",
    "![](images/paxos.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zookeeper\n",
    "* High available, scalable, distributed, configuration, consensus, group membership, leader election, naming and coordination service.\n",
    "* File API without partial reads/writes. Simple wait-free data objects organized hierarchically as in file systems.\n",
    "* Per client guarantee of FIFO execution of requests.\n",
    "* Linearizability of all requests that change zookeeper state.\n",
    "* Built using ZAB (Zookeeper Atomic Broadcast), a totally ordered broadcast protocol based on Paxos.\n",
    "* 2F + 1 server can tolerate F crash failures.\n",
    "* Client never get to see old data.\n",
    "* Client will get notified of change to data they are watching within a bounded period of time.\n",
    "* All requests from client will be proposed in order\n",
    "* All results received by a client will be consistent with results received by all other clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zookeeper servers store a copy of the data on disk. A leader is elected at startup. Followers service clients, all update go through leader. Update responses are sent when a majority of servers have persisted the change.\n",
    "![](images/zookeeper.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Protocol guarantees\n",
    "    - Sequential consistency : Updates from client will be applied in the order in which they sent\n",
    "    - Atomicity: Updates either succeed or fail. No partial result\n",
    "    - Single System Image: Client will see the same view of the service regardless of the server that it connects to\n",
    "    - Reliability: Once an update has been applied, it will persist from that time forward until a client overwrite it\n",
    "    - Timeliness: CLient's view of the system is guaranteed to be up to date within a certain bound.\n",
    "* Leader broadcast proposal for a message to be delivered. That message is assigned increasing unique id called zxid.\n",
    "* When followers received a proposal it write it to disk and send an ack to the leader as soon as the proposal is on the disk media. When leader receives ACks from quorum, the leader will broadcast commit and deliver the message locally. Followers deliver the message when they receive commit from the leader.\n",
    "* Server with the highest logged transaction gets nominated. Each server initially nominate itself. Servers poll each other to get their votes.\n",
    "* Paxos tolerate message loss and reordering, ZAB uses TCP\n",
    "* Paxos use quorums, ZAB does not.\n",
    "* Paxos' proposer believe if it is a leader it used a higher number to take over leadership from another leader. In ZAB new leaser can not takeover leadership easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
