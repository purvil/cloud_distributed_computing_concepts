{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cloud computing is a model for enabling ubiqutos, convenient, on-demand netork access to shared pool of configurable computing resources that can rapidly provisioned and released with minimal management effort or service provider interaction.\n",
    "    - Using multi tenant model with different physical and virtual resources dynamically assigned and reassigned according to consumer demand\n",
    "    - Location independence: Customer generally has no control or knowledge over exact location but may be able to specify location at a higher level of abstract, country, state or data center.\n",
    "* Elastically means rapid provision and release. Scale rapidly outward and inward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process is program in action. When we execute program it becomes process.\n",
    "* Program counter is a register that keeps track of which instruction is being executed right now. Stack is used by function to store argument and values. When function call other function it will push argument for that function on stack, when function finishes execution it will push return value on stack and caller function will retrieve it.\n",
    "* Local variable of function are also stored on stack\n",
    "* There are a variable which are assigned a memory using malloc or new such variable are stored on heap (register). It should be freed explicitly in some programming language, Ex. `delete`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DNS** : \n",
    "* Domain name system\n",
    "* Collection of server throughout the world. Helps to translate domain name to IP address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud computing\n",
    "* Cloud providers\n",
    "    - AWS (Amazon Web Services)\n",
    "        - EC2 (Elastic compute cloud) Provides computing services\n",
    "        - S3 (Simple Storage Service) Ability to store data\n",
    "        - EBS (Elastic Block Storage) Volume storage that EC2 access while they are running\n",
    "    - Microsoft azure\n",
    "    - Google Compute Engine \n",
    "    - RightScale, Salesforce, Datastax, Oracle, VMWare, Cloudera\n",
    "* **Public cloud**\n",
    "    - Provide service to any paying customers. Ex AWS\n",
    "\n",
    "* **Private cloud**\n",
    "    - Private firm owned cloud, that is accessed by employees\n",
    "\n",
    "* To install new server it takes several weeks for independent organization. Now they can rent it in 3 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud\n",
    "* Lots of storage resources with compute cycle located near by.\n",
    "* **Single site cloud**\n",
    "    - Consist of compute node, switches, connecting the racks, a network topology, storage nodes connected to network, front end to submit jobs and receiving client's request\n",
    "* **Geographically distributed cloud**\n",
    "    - Multiple such sites.\n",
    "![cloud_computing](images/cloud_computing.JPG)\n",
    "\n",
    "* Massive Scale\n",
    "    - Data centers are large with thousands of servers\n",
    "* On-demand access\n",
    "    - Pay as you go, no upfront contract\n",
    "    - Amazon EC2, AWS S3\n",
    "* Data-intensive nature\n",
    "* New cloud programming paradigm\n",
    "    - MapReduce/Hadoop, NOSQL, Cassandra, MongoDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HaaS (Hardware as a Service)\n",
    "* Get access to barebone hardware machines, do whatever you want to do.\n",
    "* SoftLayer by IBM\n",
    "\n",
    "### IaaS : Infrastructure as a Service\n",
    "* You get to access to flexible computing and storage infrastructure. Virtualization is way to achieve this. EC2, Google compute engine\n",
    "* User is provided storage, network, processing power and user can deploy and run arbitrary software (OS, application)\n",
    "* Amazon provides VMs based on intel xeon.\n",
    "\n",
    "### PaaS : Platform as a Service\n",
    "* Get access to flexible computing and storage infrastructure, coupled with software platform\n",
    "* Heroku, Google appEngine supports Java, Python,PHP, Go, automatic scaling is provided to scale number of instance of application.\n",
    "* User does not have control of OS, storage, servers, network, user can control deployed application.\n",
    "* Porgramming language, libraries service is provided by provider.\n",
    "\n",
    "### SaaS : Software as a Service\n",
    "* Access to software service, when you need that. Google docs, Gmail, Hotmail, Workday\n",
    "\n",
    "* In data intensive application, CPU utilization is no longer the most important resource metric, instead I/O is (disk and network)\n",
    "\n",
    "\n",
    "### Storage as a service\n",
    "* S3, Dropbox, Drive, onedrive\n",
    "\n",
    "### Big data as a service:\n",
    "* Amazon elastic mapreduce\n",
    "\n",
    "### Database as a service\n",
    "* Amazon SimpleDB, Azure SQL Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud is Distributed System\n",
    "* Thousands of machine in data center.\n",
    "* Millions of client machine accessing these services.\n",
    "* Server communicates among one another.\n",
    "* Client communicates with servers\n",
    "* Client communicate with each other\n",
    "* So, it is a distributed system\n",
    "* Distributed system is a collection of independent computers that appear to the user the system as a single computer.\n",
    "* Multiple computer, interconnected, shared state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce Examples\n",
    "\n",
    "#### Distributed gap\n",
    "- I/P : large set of files\n",
    "- O/P : line that match pattern\n",
    "- Map: Emits line if matches supplied pattern\n",
    "- Reduce: Copies the intermediate data to op\n",
    "\n",
    "#### Reverse web link graph\n",
    "* IP: Web graph tuple (a,b) which means page a -> page b\n",
    "* OP: For each page, list of page that link to it.\n",
    "* Map: Emits (target, source)\n",
    "* Reduce : emits (target, list(source))\n",
    "\n",
    "#### Count of URL access frequency\n",
    "* I/P: log of accessed URLs\n",
    "* O/P: For each URL % of total accesses for that URL.\n",
    "* Map <URL, 1>\n",
    "* Reduce: <URL ,count>\n",
    "* Map2: o/p <1, <URL,count>>\n",
    "* Reduce3 <url, count/total>\n",
    "\n",
    "#### Sort\n",
    "* Map task's output is already sorted using quicksort\n",
    "* Reduce tasks's input is sorted using merge sort\n",
    "* I/P Series of <key,val> pair\n",
    "* O/P sortes <vals>\n",
    "* Map output <value, _>\n",
    "* Reducer will output <value,_>\n",
    "* Here we can not use hash partition as it randomly assign keys to reducer, instead assign range based partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amazon has Elastic map reduce service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "\n",
    "-------------------------\n",
    "\n",
    "\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicast Problem\n",
    "* We have group of processes (nodes). They talk with each other.\n",
    "* Red node has info, which it wants to send to every other nodes.\n",
    "* Multicast VS Broadcast\n",
    "    - In broadcast we send message to entire network.\n",
    "    - Multicast is restricted within some group.\n",
    "![](images/multicast1.JPG)\n",
    "\n",
    "* We want fault tolerance and scalability\n",
    "    - Node may crash\n",
    "    - Packets may dropped\n",
    "    - 1000's of nodes(scalable)\n",
    "\n",
    "![](images/multicast2.JPG)\n",
    "\n",
    "* Centralized and Tree based multicast\n",
    "    - Sender has list of receivers, using for loop, go through each processes and send UDP(connection less, not reliable) or TCP protocol.\n",
    "    - If sender fails during halfway, only half will receive message\n",
    "    - Overhead is high (imagine 1000 receiver)\n",
    "    - We have tree based multicast protocol, But when node near to root fails lots of node beneath that failed node will not receive message.\n",
    "    - Build spanning tree among the process of multicast group\n",
    "    - Using ACKs or negative ACK (NACKs) we can repair multicasts not received.\n",
    "\n",
    "### Gossip protocol (Epidemic multicast)\n",
    "* Sender periodically choose b number of receivers and send them message. It is called gossip fan out. Nodes are chosen with replacement. Receiver also does the same.\n",
    "* Another variant is node which does not have message, sends PULL request to check whether other node has any new message.\n",
    "* cassandra uses gossip for maintaining membership lists\n",
    "* In initial stage of gossip if infected node dies, message might lost but very less priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because of server fail\n",
    "    - Data loss\n",
    "    - consistency loss\n",
    "\n",
    "### Membership protocol\n",
    "* We need to detect failure and disseminate failure info to other node.\n",
    "* Our goal is, at least one good process get to know about failed process in group and let other processes know.\n",
    "\n",
    "### Failure Detector\n",
    "* Completeness means each failure is detected\n",
    "* Accuracy : There is no mistaken detection (No false positive)\n",
    "* Speed : Time to detect first detection of failure\n",
    "* We need 100 % completeness.\n",
    "\n",
    "#### Centralized heart beating\n",
    "- There is a central process $P_j$, to whom every other process sends heart beat. Heart beat is sequence of incrementing number. \n",
    "* If $P_j$ does not receive heart beat from particular process within defined time period, that process is marked as failed.\n",
    "![](images/centralized_failure_detection.JPG)\n",
    "* Single point of failure\n",
    "* If 10000s of process central node might be overloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ring Heartbeating\n",
    "![](images/ring_heart_beating.JPG)\n",
    "* Each process send heat beat left AND right node.\n",
    "* Both neighbors fails then there will be a problem.\n",
    "* No central failure.\n",
    "\n",
    "#### All to All heart beat\n",
    "![](images/all_to_all.JPG)\n",
    "* Each node sends heartbeat to all processes.\n",
    "* Equal load. Well distributed\n",
    "* Completeness\n",
    "\n",
    "#### Gossip style heartbeating\n",
    "* Variant of all to all heart beating\n",
    "* Process maintains membership table for every other node. Periodically nodes select other nodes and gossip their membership list..\n",
    "* On a receipt local membership list is updated.\n",
    "* If within a timeout node does not receive heart beat from specific node then it marks it as dead node. But it will not delete entry from local list right away. It wait for another Tcleanup seconds.\n",
    "![](images/gossip_heart_beat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swim failure detector \n",
    "* Process i ping process j, if not ack received. It will give second chance but now  via different path. Process i choose other K random process and tell send them ping to let them know i need status from process j. other selected process try to ping process j, if any of them receive ack from process j. they will send ack to process i that yes process j is alive.\n",
    "* If in both try no ack from process j then process i will mark process j as failed.\n",
    "\n",
    "![](images/swim.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select each membership element once as a ping target in traversal.\n",
    "    - Round robin pinging\n",
    "    - Random permutation after each traversal\n",
    "* Each failure is detected in worst case of 2N-1 protocol period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissemination:\n",
    "* Telling other nodes that particular node is failed.\n",
    "* Multicast\n",
    "* Point to point\n",
    "* Zero extra messages: Piggyback on Failure detector message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Computing\n",
    "* Computation intensive application.\n",
    "* Workstation at different universities. They are free at night. We can use that idle resources.\n",
    "* Each site runs intra site protocol.\n",
    "* There is across site protocol like GLobus.\n",
    "* Globus decide which job will go to which site. Intra site protocol decides how task in the job will be executed.\n",
    "\n",
    "#### Condor (HTCondor)\n",
    "* High throughput computing system\n",
    "* When work station is free it ask center server for tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peer to Peer system\n",
    "* Example: Napster, Gnutella, Fasttrack, BitTorent, Chord, Pastry\n",
    "\n",
    "### Napster\n",
    "* Each users are napster client (peer)\n",
    "* When peer upload file, file will stay on local machine\n",
    "* Namster.com server stores file info and peer info where the file is located.\n",
    "![](images/napster.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For serach\n",
    "    - Client send server keywords to search with\n",
    "    - Server returns lists of hosts <ip_address, portnum> tuples to client\n",
    "    - Client ping each host and find the best host\n",
    "* All communication uses TCP.\n",
    "* Server are highly busy, single point of failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gnutella\n",
    "* ELiminate servers\n",
    "* Client machines search and retrieve amongst themselves\n",
    "* Peer has some neighbors meaning they know about ip address of their neighbors and can send message.\n",
    "![](images/gnutella.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are messages like,\n",
    "    - Query (Search)\n",
    "    - QueryHit (Response to serach)\n",
    "    - Ping : To probe network for other peer\n",
    "    - Pong: Reply to ping contains address of another peer\n",
    "    - Push: used to initiate file transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query message\n",
    "![](images/msg.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/query_payload.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/query.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To avoid duplicate transmission, each peer maintain list of recently received messages.\n",
    "* Query forwarded to each neighbors except from which received.\n",
    "* Each query forwarded only once (identified by descriptor id)\n",
    "* Queryhit routed back only to peer from which query received with same DescriptorId\n",
    "* Reqestor chooses best queryhit responder.\n",
    "* Respnder then replies with file packets.\n",
    "* To overcome firewall, requestor sends push message to responder asking for file transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/firewall.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Peer intiate ping periodically.\n",
    "* Pong replies used to update set of neighboring peers\n",
    "* To keep neighbor lists fresh in spite of peers joining, leaving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/napster_gnutella.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastTrack\n",
    "* Some nodes are super nodes. They stores directory listing a subset of nearby, similar to napstar.\n",
    "* Any peer can become supernode provided it has enough reputation.\n",
    "* A peer searches by contacting near by supernode.\n",
    "\n",
    "\n",
    "### BitTorrent\n",
    "![](images/bit_torrent.JPG)\n",
    "* Find tracker for file you are searching. Tracker maintain list of some nodes which has that file.\n",
    "* Peer download blocks from different peers.\n",
    "* File split into block.\n",
    "* Download local rarest first : Prefer early download of blocks that are least replicated among others\n",
    "* Provide blocks to neighbors that provided it the best download rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chord\n",
    "* Hash table allows you to insert, lookup and delete objects with keys\n",
    "* A distributed hash table allows you to do the same in a distributed setting.\n",
    "* Intelligent choice of neighbors to reduce latency and message cost of routing.\n",
    "* Uses consistent hashing on node's address\n",
    "    - SHA-1(ip_address, port) -> 160 bit string (Secure Hash Algorithms)\n",
    "    - Truncated to m bits\n",
    "    - Called peer id (number between 0 and $2^m$-1)\n",
    "    - Can then map peers to one of $2^m$ logical points on a circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Every node knows about it's successor\n",
    "![](images/chord.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each peer maintain neighbors\n",
    "    - One of the neighbor is it's clock wise successor. 16 knows about ip and port of 32. 32 knows about 45\n",
    "* Similarly each node knows its counter clock wise successor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finger tables\n",
    "* Useful to route the query very quickly.\n",
    "![](images/chord_fingerprint.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* File name are also mapped using the same consistent hash function.\n",
    "    - SHA-1(filename) = 160 bit string key\n",
    "* File is stored at first peer with id greater than or equal to its key % $2^m$.\n",
    "* File xyz maps to key K42 is stored at first peer with id greater than 42.\n",
    "* Consistent hashing => with K keys and N peers,each peer stores, each node has O(K/N) keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Search\n",
    "![](images/chord_search.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Search takes O(logN) time. At each step distance between query and peer with file reduces by a factor of at least 2.\n",
    "* Replicate file to load balance and failure protection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* New peer join by contacting well known introducer using DNS.\n",
    "* It gives IP address of some peer in the system.\n",
    "* Introducer directs N40 to N45 and N32/ N32 updates its successor to N40. N40 initialize successor to N45 and init fingers from it. N40 periodically talk to neighbor to update finger table.\n",
    "* This virtual ring and consistent hashing is used in Cassandra, Riak, Voldemort and DynamoDB and other key val store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key value store\n",
    "* Map key to given value\n",
    "* Example tweet id to info about tweet\n",
    "* item umber to info about item\n",
    "* Flight number to info about flight\n",
    "* Account number to info about it.\n",
    "* Data is large and unstructured (Hard to design schema)\n",
    "* Lots of read and writes from multiple clients\n",
    "* Relational db are not good for write heavy work load.\n",
    "* Foreign key rarely needed, join is infrequent.\n",
    "* We need speed, avoid single point of failure, incremental scalability, scale out(horizontal)\n",
    "* NoSQL key value database has 2 main operations, get(key) and put(key,val)\n",
    "* Relational database: table, cassandra: column families, HBase: table, MongoDB:collection\n",
    "* But here table is not like RDBMS< some column may missing from some roes.\n",
    "* No foreign keys and joints not supported.\n",
    "\n",
    "### Column oriented storage\n",
    "* RDBMS store an entire row together.\n",
    "* NOSql system typically store a column together or a group of columns.\n",
    "    - Entries within a column are indexed and easy to locate using key.\n",
    "* Range searches within a column is fast so we dont need to fetch entire database.\n",
    "    * Get me all blog_id from blog table that were updated within the past month.\n",
    "        - Search in last updated column, fetch corresponding blog_id column\n",
    "        - Don't need to fetch other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cassandra\n",
    "* Distributed key value store.\n",
    "* Designed at facebook\n",
    "* Facebook use for inbox search, netflix use for current position in video \n",
    "* How do you decide which server a key-value reside on?\n",
    "    - Cassandra place server in virtual ring.\n",
    "![](images/cassandra.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Key is stored at successor server.\n",
    "* Key mapped to 13, is stored at 16,32,45\n",
    "* CLient send request to any server.\n",
    "* Each datacenter has own virtual ring.\n",
    "* THere is not routing or finger table like Chord, coordinator knows where every key are stored.\n",
    "* Mapping from key to server is called partitioner.\n",
    "* Replication strategy\n",
    "    - SimpleStrategy  : use partitioner\n",
    "        - RandomPartitioner: Chord like hash partitioning. Keys are hash at point of node  and stored at successor.\n",
    "        - ByteOrderedPartitioner : Assign ranges of keys to servers. Easier for range queries, get me all tweets for user starting with [a-b]\n",
    "    - NetworkTopologyStrategy : For multi data center deployment\n",
    "    - 2 replicas per DC\n",
    "    - 3 replicas per DC.\n",
    "    - per DC\n",
    "        - 1st replica placed according to partitioner\n",
    "        - then go clockwise around ring until you hit a different rack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snitches\n",
    "* maps ip to rack and data centers. Configured at `cassandra.yaml` file\n",
    "* SimpleSnitch: Unaware of rolology (rack-unaware)\n",
    "* RackInferring: Assumes topology of network by octet of server's ip address\n",
    "    - 101.102.103.104 = x.DC octate.Rack octet.node octet\n",
    "* PropertyFileSnitch: use a config file\n",
    "* EC2 Snitch\n",
    "    - EC2 region = DC\n",
    "    - Availability zone = rack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writes in Cassandra\n",
    "* Need to be lock free and fast. CLient sends write to 1 coordinator in cassandra cluster. Coordinator may be per key, per client or per query. Per key coordinator ensures that writes for the key are serialized.\n",
    "* Coordinator uses partitioner to send query to all replica nodes responsible for key. When X number of replicas respond coordinator returns an ack to the client.\n",
    "* If any replica is down, coordinator writes to all other replicas and keeps the write locally until down replica comes back up.\n",
    "* Above 2 scenario is called hjnted handoff\n",
    "* When all replica are down, coordinator buffers writes.\n",
    "* One ring per DC. per-DC coordinator elected to coordinate with other Dcs.\n",
    "* Election done via zookeeper (variant of Paxos).\n",
    "* On receiving write, node log i in disk commit log (for failure recovery)\n",
    "* Make changes to appropriate memtables\n",
    "    - Memtable = in memory representation of multiple key value pairs\n",
    "    - Cache that can be searched by key\n",
    "    - Write back cache as opposed to write through\n",
    "* When memtable is full or old, flush to disk.\n",
    "* SSTable(Sorted String table)-list of key value pairs sorted by key\n",
    "* Index file - An SSTable of (key,position in data sstable) pairs\n",
    "\n",
    "\n",
    "#### Bloom Filter\n",
    "* Compact way of representing a set of items. so checking for existence in set is cheap.\n",
    "* Some probability of false positive : item not in set still check true as being in set.\n",
    "* Never false negatives\n",
    "![](images/bloom_filter.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially all bits are 0\n",
    "* There are set of hash functions.\n",
    "* On every key k bloom filter is applied and at output bit position we set the bit to 1.\n",
    "* On check if present return true if all hashed bit is set.\n",
    "* False positive rate low,\n",
    "    - k = 4 hash function\n",
    "    - 100 items\n",
    "    - 3200 bits\n",
    "    - False positive rate = 0.02%\n",
    "* Example per 3 hash function h1, h2, h3 = $(x^2+ x^3)*i$ mod m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compaction\n",
    "* Data updates accumulated over time in different SSTables and logs need to be compacted.\n",
    "* The process of compaction merges SSTables that is merging updated for a key.\n",
    "* Run periodically and locally for each server.\n",
    "\n",
    "#### Delete\n",
    "* Don't delete item right away.\n",
    "* Add to tombstone to the log\n",
    "* Eventually, when compaction encounters tombstone it will delete item\n",
    "\n",
    "#### Read:\n",
    "* Client contact coordinator. Coordinator contact X replicas (in same rack). It send read to replica that respond fastest in the past. When X replica responds. Coordinator returns the latest time stamped value from among those X.\n",
    "* Coordinator also fetches value from other replicas\n",
    "    - Checks consistency in background, initiating a read repair if any 2 values are different. This mechanism eventually bring all replicas to up to date state.\n",
    "    - A row may be split across multiple SSTable, read need to touch all SSTables, read slower than write.\n",
    "    \n",
    "#### Membership\n",
    "* Any server in cluster can be the coordinator.\n",
    "* So every server needs to maintain list of all other servers that are currently in server\n",
    "* Cassandra maintain membership list at each server. Gossip style membership list.\n",
    "![](images/cassandra_membership.JPG)\n",
    "* Timeout is set based on underlying network and failure behavior. For each server there is a inter arrival times of gossip message. And using probability distribution of last arrived messages, specific server set time out of every other servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP Theorem\n",
    "* In a distributed system you can satisfy 2 out of 3 property.\n",
    "1. **Consistency**: All nodes see same data at any time, or reads return latest written value by any client\n",
    "    - When you access bank or investment account via multiple client(laptop, phone), we want the updates done from one client to be visible to other.\n",
    "    - When thousands of customers are looking to book a flight, all updates from any client should be accessible by other clients.\n",
    "2. **Availability**: The system allows operations all the time and operation return quickly\n",
    "    - Not quick response can cause huge loss to company. and service level agreement may be violated.\n",
    "3. **Partition tolerance**: The system continues to work in spite of network partition.\n",
    "    - Partition can occur, due to internet router outage, under sea cable cuts, DNS not working, rack switch outage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Casandra choose availability over consistency. It provides eventual consistency. RDBMS provides strong consistency over availability.\n",
    "![](images/cap.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eventual Consistency\n",
    "* If all writes stops to key then its values will converge eventually\n",
    "* If write continues, system always try to keep converging.\n",
    "* May still return stale value to client.\n",
    "* Amazon DynamoDB and linkedin's voldemort also eventual consistent\n",
    "\n",
    "#### ACID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDBMS provides ACID (atomocity, consistency, isolation and durability)\n",
    "* Cassandra has consistency level\n",
    "    - Client are allowed to choose consistency level for each operation(read/write)\n",
    "        - ANY: any server, fastest\n",
    "        - ALL : all replicas, strong consistency, but slowest\n",
    "        - One: at least 1 replica : faster than All.can not tolerate failure\n",
    "        - QUORUM: quorum across all replicas in all data centers. Global consistency\n",
    "![](images/cassandra1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency model\n",
    "#### Per key sequential\n",
    "* Per key coordinator. All operation has global order\n",
    "\n",
    "#### Red-blue\n",
    "* Rewrite client transaction to separate ops into red ops vs blue ops. Blue ops can be executed in any order across DCs\n",
    "* Red ops need to be executed in same order at each DC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBase\n",
    "* Prefer consistency over availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "* Why synchronization?\n",
    "- Want to catch bus at 6:00 my clock is off by 15 minutes miss the bus\n",
    "- clock fast by 15 minutes : wait\n",
    "* Time synchronization needed both for Correctness and Fairness.\n",
    "* Cloud airline reservation system\n",
    "    - Server A receive client request to purchase last ticket on flight ABC 123\n",
    "    - Server A's timestamp purchase using local clock 9h:15m:32.45s and log it. Replies ok to client\n",
    "    - Server A send message to server B saying flight full.\n",
    "    - B enters flight full in own clock at 9h:10m:10.11s.\n",
    "    - Server C queries A and B's log. Is confused that a client purchased a ticket at A after flight become full at B.\n",
    "    \n",
    "* Each have own clocks. An asynchronous distributed system consists of number of processes.\n",
    "* Each process has a state. Each process take action to change its state, Each process has local clock, event within a process can be assigned a time stamp. and ordered linearly.\n",
    "* We also need to know time order of events across different processes.\n",
    "* **Clock skew**: Relative difference in clock values of 2 processes. Like distance between 2 vehicle  on road.\n",
    "* **Clock drift**: Relative difference in clock frequencies of 2 processors. Difference in speed of 2 vehicles.\n",
    "* Non 0 clock skew means clock is not synchronized.\n",
    "* non 0 clock drift causes skew to increase\n",
    "\n",
    "\n",
    "##### How to synchronize clocks?\n",
    "* External Synchronization\n",
    "    - Each process's clock is within a bound D of a well known clock S external to the group.\n",
    "    - External clock may be connected to UTC or atomic clock.\n",
    "    - Ex.NTP\n",
    "    - External Synchronization with D means internal Synchronization with 2 * D\n",
    "* Internal Synchronization\n",
    "    - Every pair of processes in group have clock within bound D.\n",
    "    - Internal synchronization does not imply external synchronization. Entire group can drift away from external clock.\n",
    "    \n",
    "#### Cristian's algorithm\n",
    "* External synchronization\n",
    "* All process P synchronize with a time server S.\n",
    "* By the time response message is received a P, time has moved on. P's time set to t is inaccurate. It is a function of message latencies.\n",
    "* P measures the round trip time RTT of message exchange.\n",
    "* Suppose we know the minimum p to S latency = min1\n",
    "* and min S to P latency = min2\n",
    "* Actual time at P when it receives responce is [t+min2, t+RTT-min1].\n",
    "* P sets its time to halfway through this interval = t + (RTT+min2-min1)/2\n",
    "\n",
    "### NTP\n",
    "* External synchronization\n",
    "* Network time protocol\n",
    "* NTP server are organized in a tree.\n",
    "* Each client = a leaf of a tree\n",
    "* Each node synchronized with its parent.\n",
    "![](images/ntp.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ntp2.JPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Client use all 4 values to set clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lamport timestamp (logical timestamp)\n",
    "* Ordering events in distributed system\n",
    "* As long as time stamp obey causality, that would work\n",
    "* If event A cause event B then timestamp(A) < timestamp(B). I enter a house only after I unlock it. You receive a letter only after I send it.\n",
    "* Happens-Before denoted a -> b\n",
    "* On same process a->b then time(a) < time(b)\n",
    "* If p1 sends m to p2 send(m) -> receive(m)\n",
    "* a->b and b->c then a->c\n",
    "* Each process us local counter which is an integer. Initial value is 0.\n",
    "* Process increment counter when a send or an instruction happens in it. Counter is assigned to event as its timestamp.\n",
    "* A send even carries its timestamp\n",
    "* Receive event the counter is updated by max(local counter, message counter) + 1\n",
    "![](images/lamport.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector clock\n",
    "* Each process uses a vector of integer clock.\n",
    "* In lampor we can not distinguish concurrent event By using more space in vector timestamp we can identify concurrent events.\n",
    "![](images/vector_timestamp.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When you send message, increment your entry, receive message increment your entry. and for sender process's take max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global snapshot\n",
    "* In cloud each application or service is running on multiple servers.\n",
    "* Server handling concurrent events and interacting with each other.\n",
    "* We need global photo of the system, to,\n",
    "    - Checkpointing: So we can restart application on failure\n",
    "    - Garbage collection : Objects at servers that do not have any other pointers to them\n",
    "    - Deadlock detection: Useful in db transaction\n",
    "    - Termination of computation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Global snapshot = individual state of each process on  the distributed system + individual state of each communication channel in the distributed system\n",
    "* First we need to synchronize all the clock of processes. Ask all processes to record their states at known time t\n",
    "- Time synchronization is inaccurate to calculate.\n",
    "- But the causal change is enough\n",
    "- Whenever event happens anywhere in the system the global state changes\n",
    "    - Process receive message\n",
    "    - Process sends message\n",
    "    - Process takes step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global snapshot algorithms\n",
    "* We have N process in the system, There are 2 unidirectional communication channel between each process pair\n",
    "* Communication channel are FIFO\n",
    "\n",
    "#### Chandy lamport global snapshot algorithm\n",
    "* First initiator Pi records its own state.\n",
    "* Initiator process creates special message called Marker message.\n",
    "* For all other process Pjs,  Pi sends marker message to outgoing channel Cij\n",
    "* Pi starts recording of incoming messages on each of the incoming channel at Pi.\n",
    "* Whenever other process receive the marker message on an incoming channel\n",
    "    - Process record its own state first.\n",
    "    - Mark the state of channel on which it received the message as empty.\n",
    "    - Process sends marker message to all other channel on outgoing channel.\n",
    "    - Start recording the incoming message on each of the incoming channels at process.\n",
    "* Algorithm will terminate when all process have received a marker and record their own state.\n",
    "* All process have received a marker on all the N-1 incoming channels at each to record the state of channels\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liveness\n",
    "* guarantee that something good will happen eventually.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicast Ordering\n",
    "* Multicast is message sent to a group of process, broadcast is message sent to all processes, Unicast is message sent from 1 sender to 1 receiver.\n",
    "* We need multicast reliable meaning all process in a group receive message and in order.\n",
    "* Example\n",
    "    - Cassandra use multicast for message between group of replicas for a key. Write or read to the key are multicast within the replica group.\n",
    "    - Membership info is multicast across all server in cluster.\n",
    "    - Multicast to group of client interested in scores\n",
    "1. FIFO ordering\n",
    "    - Multicast from each sender are received in the order they are sent, at all receivers\n",
    "    - Dont worry about multicast from different senders\n",
    "    - Each receiver maintains per sender sequence number. Process pi maintain vector of 1 to N for all total N processes. Initially all are 0. Pi[j] is latest sequence number Pi received from Pj. if j = i, is sequence number of Pi's message sent out.\n",
    "    - Send multicast at process Pj , set Pj[j] += 1. Include P[j] in message as seq. number.\n",
    "    - Receive multicast: If Pi receives a multicast from Pj with seq number S in message\n",
    "        - if (S == Pi[j] + 1) then deliver message to application set Pi[j] += 1\n",
    "        - Else buffer this multicast until above condition is true\n",
    "2. Causal ordering\n",
    "    - Multicast whose send events are causally related, must be received in the same causality obeying order at all receiver\n",
    "    - Used at social network, bulletin boards, comments on website.\n",
    "        - A friend see your message m and she post response to it m'.\n",
    "        - If other friends receive m' before m ther get confused.\n",
    "        - If 2 friends post m'' and m''' concurrently then they can be sen in any order at receivers.\n",
    "    - Each receiver maintains a vector of per sender seq. number\n",
    "    - Initially all 0\n",
    "    - Send multicast at process Pj, set Pj[j] += 1\n",
    "    - Include new entire vector in multicast message as its seq number.\n",
    "    - Receiver:\n",
    "        - If Pi receive a multicast from Pj with vector M[1...N] in message, buffer it until\n",
    "            - This message is next one Pi is expecting from Pj\n",
    "                - M[j] = Pi[j] + 1\n",
    "            - All multicast, anywhere in the group, which happended before M have been received at Pi.\n",
    "                - For all K != j : M[k] <= Pi[k]\n",
    "            - When above condition is setidfied it deliver M to application and Pi[j] = M[j]\n",
    "3. Total Ordering:\n",
    "    - Atomic broadcast\n",
    "    - Does not pay attention to order.\n",
    "    - Ensure all receivers receive all multicasts in same order. Ex. All stock brocker should get message in same order.\n",
    "    - Special process elected as leader or sequencer\n",
    "    - Send multicast at process Pi\n",
    "        - Send multicast message M to group and sequencer.\n",
    "        - Sequencer maintain global seq. number\n",
    "        - When it receive multicast message M, it set S =S + 1 and multicast <M,S>\n",
    "        - When Pi receive multicast, It maintain local global seq number Si\n",
    "        - If Pi receives a multicast M from Pj, it buffers it unrill it both\n",
    "            - Pi receives <M, S(M)> from sequencer\n",
    "            - Si + 1 = S(M)\n",
    "            - hen deliver to the application and set Si = Si + 1\n",
    "* Also hybrid variants are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Synchrony/ View Synchrony\n",
    "* Attempts to preserve multicast ordering and reliability inspite of failures\n",
    "* Combines a membership protocol with a multicast protocol.\n",
    "* Each process maintains membership list called a View.\n",
    "* Update to membership list is called view change, process join, fail, leave.\n",
    "* Virtual synchrony guarantees that view change are delivered in the same order at all correct process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus Problem\n",
    "* A group of server attempting\n",
    "    - Reliable Multicast : Make sure all of them receive the same updates in the same order as each other\n",
    "    - Membership/Failure Detection: To keep their own local lists where they know about each other, and when anyone leaves or fail everyone is updates simultaneously\n",
    "    - Leader election: Elect leader among them and let everyone in the group know about it\n",
    "    - Mutual Exclusion: To ensure mutually exclusive access to a critical resources like a file.\n",
    "* All the process at different server, attempt to coordinate with each other and reach agreement on leader election, ordering of message, up/down status of failed process, Who has access to critical resource.\n",
    "* All are consensus problem.\n",
    "* We have N process, each process has input variable either 0 or 1, output variable in 0 or 1. Once output variable is set, process can not change it.\n",
    "* Design a protocol such as at the end all output variable are either 0 or 1.  That is the consensus problem.\n",
    "\n",
    "\n",
    "#### Two different model of distributed system\n",
    "1. Synchronous system model\n",
    "    - Each message is received within bounded time\n",
    "    - Drift of each process local clock has know bound\n",
    "    - Each step in a process takes time within some bound.\n",
    "    - Example: Collection of processors connected by communication bus, supercomputer, multicore machine\n",
    "2. Asynchronous system\n",
    "    - No bound on process execution\n",
    "    - The drift rate of clock is arbitrary\n",
    "    - No bound on message transmission delay\n",
    "    - Ex, Internet, sensor network\n",
    "    \n",
    "    \n",
    "#### Paxos\n",
    "* Very hard to solve consensus in Asynchronous model. We can not distinguish failed process from very very slow process. So rest of the alive process may stay ambivalent\n",
    "* Paxos is most popular consensus solving algorithm. Zookeeper, Google Chubby use it.\n",
    "* Paxos has a round, each round has ballot id.\n",
    "* Rounds are asynchronous.\n",
    "    - If you are in round j and hear a message from round j+1, abort everything and move to round j + 1\n",
    "    - Use time out\n",
    "* Each ound is broken in phase\n",
    "    - Phase 1: Leader is elected (Election\n",
    "    - Phase 2 : Leader proposes a value, processes ack. (Bill)\n",
    "    - Phase 3: Leader multicasts final value. (Law)\n",
    "* Election:\n",
    "    - Ptential leader chooses a unique ballot id, higher than seen anything so far.\n",
    "    - Sends to all process\n",
    "    - Process wait, respond once to highest ballot id\n",
    "    - If potential leader sees highest ballot id, it cant be leader\n",
    "    - Paxos is tolerant to multiple leader, but 1 leader is idle case\n",
    "    - Process log received ballot ID on disk\n",
    "* If process has in a previous round decided on a value v' then it includes v' in its response.\n",
    "* If majority respond OK then you are the leader.\n",
    "* Leader sends proposed value v to all, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
